{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bba1c7e",
   "metadata": {},
   "source": [
    "# Predicting Individual Awards: The Sportsmanship Award\n",
    "\n",
    "**Project Context**\n",
    "This notebook addresses **Task (c)** of the project description: \"Who won each of the individual awards\". Specifically, this analysis focuses on predicting the **Sportsmanship Award**.\n",
    "\n",
    "**The Logic of Sportsmanship**\n",
    "Unlike MVP or Scoring titles which are purely performance-based, the Sportsmanship Award honors players who exemplify ethical behavior and integrity. In basketball analytics, this is typically modeled as:\n",
    "* **High Availability:** Playing significant minutes (essential to the team).\n",
    "* **Low Conflict:** Committing very few personal fouls or technical fouls relative to playing time.\n",
    "* **High Performance:** Generally, awards go to recognized, high-quality players (All-Stars), not bench warmers.\n",
    "\n",
    "**Data Sources**\n",
    "* **`players_teams`**: Granular statistics (Minutes, PF, DQ, etc.) to model behavior.\n",
    "* **`awards_players`**: Historical labels for training (Who won previously?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34ce99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "# --- 1. Setup & Data Loading ---\n",
    "initial_path = '../../data/initial_data/' \n",
    "test_path = '../../data/test_data/'\n",
    "\n",
    "def load_and_combine(filename):\n",
    "    \"\"\"\n",
    "    Loads historical data and appends test data if available.\n",
    "    Ensures Year 11 is part of the dataframe for feature generation.\n",
    "    \"\"\"\n",
    "    # Load historical data\n",
    "    df = pd.read_csv(f\"{initial_path}{filename}\")\n",
    "    \n",
    "    # Try to load test data\n",
    "    try:\n",
    "        df_test = pd.read_csv(f\"{test_path}{filename}\")\n",
    "        # Concatenate\n",
    "        df = pd.concat([df, df_test], ignore_index=True)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Note: No test file found for {filename}\")\n",
    "        pass\n",
    "    return df\n",
    "\n",
    "# Load tables\n",
    "players_teams = load_and_combine('players_teams.csv')\n",
    "awards = pd.read_csv(f\"{initial_path}awards_players.csv\")\n",
    "# We load teams just for context (wins/losses)\n",
    "teams = load_and_combine('teams.csv')\n",
    "\n",
    "print(f\"Players Stints Loaded: {players_teams.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51b49a1",
   "metadata": {},
   "source": [
    "### 2. Feature Engineering: Quantifying \"Integrity\"\n",
    "To predict \"Sportsmanship,\" we must quantify behavior. We derive a custom metric called the **Discipline Ratio**.\n",
    "\n",
    "* **`discipline_ratio`**: $\\frac{\\text{Minutes Played}}{\\text{Fouls} + (\\text{Disqualifications} \\times 50) + 1}$\n",
    "    * This rewards players who can stay on the floor for long periods without fouling.\n",
    "    * We heavily penalize **Disqualifications (DQ)** (ejected from game) as they are the antithesis of sportsmanship.\n",
    "\n",
    "**Handling the \"Zero Feature Trap\"**\n",
    "A common issue in forecasting is missing data for the test year (Year 11) or players missing a prior season due to injury. If we rely solely on $t-1$ stats, these players get \"0\" scores.\n",
    "* **The Fix**: We calculate **Cumulative Career Averages** using expanding windows. If a player's previous season stats are missing, we fallback to their career baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e3fc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature Engineering: Integrity & Base Performance ---\n",
    "\n",
    "# 1. Aggregation (Handle players with multiple stints in one year)\n",
    "agg_cols = ['GP', 'minutes', 'points', 'rebounds', 'assists', 'steals', \n",
    "            'blocks', 'turnovers', 'PF', 'fgMade', 'fgAttempted', 'dq']\n",
    "\n",
    "# Sum stats to get total season footprint\n",
    "df_season = players_teams.groupby(['playerID', 'year'])[agg_cols].sum().reset_index()\n",
    "\n",
    "# 2. Calculate Derived Metrics (Integrity + Efficiency)\n",
    "# \"Lady Byng\" Ratio: Reward high minutes with low fouls. Penalty: DQ counts as 50 fouls.\n",
    "df_season['discipline_ratio'] = df_season['minutes'] / (df_season['PF'] + (df_season['dq'] * 50) + 1)\n",
    "df_season['dq_severity'] = df_season['dq'].apply(lambda x: 3 if x > 0 else 0)\n",
    "\n",
    "# Efficiency (PER Proxy) - Calculated on RAW data to enable career averaging\n",
    "df_season['missed_fg'] = df_season['fgAttempted'] - df_season['fgMade']\n",
    "df_season['efficiency'] = (\n",
    "    df_season['points'] + df_season['rebounds'] + df_season['assists'] + \n",
    "    df_season['steals'] + df_season['blocks']\n",
    ") - (df_season['missed_fg'] + df_season['turnovers'] + df_season['PF'])\n",
    "\n",
    "# 3. THE FIX: Cumulative Career Fallbacks\n",
    "# We calculate career averages to fill gaps if 'prev_year' is empty (Zero Feature Trap)\n",
    "df_season.sort_values(['playerID', 'year'], inplace=True)\n",
    "df_season['career_discipline'] = df_season.groupby('playerID')['discipline_ratio'].expanding().mean().reset_index(level=0, drop=True)\n",
    "df_season['career_efficiency'] = df_season.groupby('playerID')['efficiency'].expanding().mean().reset_index(level=0, drop=True)\n",
    "\n",
    "# 4. Create \"Previous Season\" Lagged Features\n",
    "df_lagged = df_season.copy()\n",
    "df_lagged['year'] = df_lagged['year'] + 1  # Shift forward (Year 10 stats become Year 11 features)\n",
    "\n",
    "# Rename to indicate history\n",
    "cols_to_lag = agg_cols + ['discipline_ratio', 'dq_severity', 'efficiency', 'career_discipline', 'career_efficiency']\n",
    "lag_cols = {col: f'prev_{col}' for col in cols_to_lag}\n",
    "df_lagged.rename(columns=lag_cols, inplace=True)\n",
    "\n",
    "# Merge back to the master list of players available in the Target Year\n",
    "master_df = df_season[['playerID', 'year']].drop_duplicates().copy()\n",
    "master_df = master_df.merge(df_lagged, on=['playerID', 'year'], how='left')\n",
    "\n",
    "# 5. THE FIX: Apply Fallbacks\n",
    "# If prev_efficiency is NaN (missed last season), use career average instead of 0\n",
    "master_df['prev_efficiency'] = master_df['prev_efficiency'].fillna(master_df['prev_career_efficiency'])\n",
    "master_df['prev_discipline_ratio'] = master_df['prev_discipline_ratio'].fillna(master_df['prev_career_discipline'])\n",
    "\n",
    "# Fill remaining NaNs (True Rookies) with 0\n",
    "master_df.fillna(0, inplace=True)\n",
    "\n",
    "print(\"Integrity & Performance features calculated with Career Fallbacks.\")\n",
    "display(master_df[['playerID', 'year', 'prev_discipline_ratio', 'prev_efficiency']].tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d06c2b",
   "metadata": {},
   "source": [
    "### 3. Feature Engineering: Reputation & Award History\n",
    "Awards are rarely given in a vacuum; reputation matters. Voters often favor established veterans or previous winners.\n",
    "\n",
    "We engineer specific \"Status\" features:\n",
    "* **`years_in_league`**: Seniority often correlates with respect.\n",
    "* **`cumulative_all_stars`**: A proxy for \"Star Power.\" Voters are more likely to notice a polite All-Star than a polite role player.\n",
    "* **`prev_sp_wins`**: History of winning the Sportsmanship award (Dynasty effect).\n",
    "\n",
    "**Target Definition: Graded Relevance**\n",
    "We treat this as a **Learning to Rank (LTR)** problem with a 3-tier target:\n",
    "* **3 (Winner)**: The player who actually won the award.\n",
    "* **1 (All-Star)**: A proxy for \"Nominee\" or \"Relevant Player.\" This helps the model learn to distinguish \"Good Players\" from \"Winners,\" rather than just \"Random Players\" vs. \"Winners.\"\n",
    "* **0 (None)**: The rest of the league."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b276aacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature Engineering: Reputation & Targets ---\n",
    "\n",
    "# 1. Years in League (Veterancy Bias)\n",
    "df_season.sort_values(['playerID', 'year'], inplace=True)\n",
    "df_season['years_in_league'] = df_season.groupby('playerID').cumcount() + 1\n",
    "\n",
    "# 2. Cumulative Awards (Dynasty Effect)\n",
    "# Did they win Sportsmanship before?\n",
    "sp_winners = awards[awards['award'].str.contains('Sportsmanship')].copy()\n",
    "sp_winners['is_sp_winner'] = 1\n",
    "\n",
    "cum_sp = sp_winners.groupby(['playerID', 'year'])['is_sp_winner'].sum().groupby('playerID').cumsum().reset_index()\n",
    "cum_sp['year'] += 1 \n",
    "cum_sp.rename(columns={'is_sp_winner': 'prev_sp_wins'}, inplace=True)\n",
    "\n",
    "# All-Star Selections (Visibility)\n",
    "all_stars = awards[awards['award'].str.contains('All-Star')].copy()\n",
    "all_stars['is_all_star'] = 1\n",
    "cum_as = all_stars.groupby(['playerID', 'year'])['is_all_star'].sum().groupby('playerID').cumsum().reset_index()\n",
    "cum_as['year'] += 1\n",
    "cum_as.rename(columns={'is_all_star': 'cumulative_all_stars'}, inplace=True)\n",
    "\n",
    "# 3. Merge All Features\n",
    "final_df = master_df.merge(df_season[['playerID', 'year', 'years_in_league']], on=['playerID', 'year'], how='left')\n",
    "final_df = final_df.merge(cum_sp, on=['playerID', 'year'], how='left')\n",
    "final_df = final_df.merge(cum_as, on=['playerID', 'year'], how='left')\n",
    "final_df.fillna(0, inplace=True)\n",
    "\n",
    "# 4. THE FIX: Graded Relevance Target\n",
    "# Winner = 3\n",
    "target_awards = awards[awards['award'].str.contains('Sportsmanship')][['playerID', 'year']].copy()\n",
    "target_awards['is_winner'] = 3 \n",
    "\n",
    "# All-Star = 1 (Proxy for \"Nominee/Good Player\")\n",
    "as_awards = awards[awards['award'].str.contains('All-Star')][['playerID', 'year']].copy()\n",
    "as_awards['relevance_as'] = 1\n",
    "\n",
    "# Merge Targets\n",
    "final_df = final_df.merge(target_awards, on=['playerID', 'year'], how='left')\n",
    "final_df = final_df.merge(as_awards, on=['playerID', 'year'], how='left')\n",
    "\n",
    "# Combine: Winner (3) overrides All-Star (1), others are 0\n",
    "final_df['is_winner'] = final_df['is_winner'].fillna(final_df['relevance_as']).fillna(0)\n",
    "final_df.drop(columns=['relevance_as'], inplace=True)\n",
    "\n",
    "# Filter: Remove Rookies (Year 1)\n",
    "final_df = final_df[final_df['years_in_league'] > 1].copy()\n",
    "\n",
    "print(f\"Final Dataset Shape: {final_df.shape}\")\n",
    "print(\"Target Distribution (0=None, 1=AllStar, 3=Winner):\")\n",
    "print(final_df['is_winner'].value_counts())\n",
    "display(final_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a0e2c5",
   "metadata": {},
   "source": [
    "### 4. Modeling Strategy: SOTA Ensemble\n",
    "Predicting a single winner out of hundreds of players is highly volatile. To stabilize predictions, we employ an ensemble of three distinct architectures:\n",
    "\n",
    "1.  **XGBoost Ranker (`rank:ndcg`)**:\n",
    "    * Specializes in optimizing the list order directly.\n",
    "    * Tuned with high regularization to prevent overfitting to historical outliers.\n",
    "\n",
    "2.  **LightGBM Ranker (`lambdarank`)**:\n",
    "    * Uses a different gradient boosting implementation that often captures different signal patterns.\n",
    "    * Explicitly defined label gains (0, 1, 10) to heavily prioritize finding the \"Winner.\"\n",
    "\n",
    "3.  **Random Forest Regressor**:\n",
    "    * Acts as a stabilizer. While Rankers care about *relative* order, the Random Forest looks at *absolute* feature strength. It prevents the rankers from hallucinating a winner in a weak year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b16955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestRegressor, VotingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# --- 1. Model Definitions (SOTA Ensemble) ---\n",
    "\n",
    "# A. XGBoost Ranker (The Specialist)\n",
    "# Tuned for small data: deeper trees but high regularization\n",
    "xgb_model = xgb.XGBRanker(\n",
    "    objective='rank:ndcg',\n",
    "    eval_metric='ndcg@3',\n",
    "    learning_rate=0.03,    # Slower learning for better generalization\n",
    "    n_estimators=1000,\n",
    "    max_depth=5,\n",
    "    min_child_weight=2,    # Prevents overfitting to single outliers\n",
    "    lambdarank_pair_method='topk',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# B. LightGBM Ranker (The Speedster)\n",
    "# FIXED: Label gain must match the max label index (0, 1, 3 -> size 4)\n",
    "lgb_model = lgb.LGBMRanker(\n",
    "    objective='lambdarank',\n",
    "    metric='ndcg',\n",
    "    learning_rate=0.03,\n",
    "    n_estimators=1000,\n",
    "    max_depth=4,\n",
    "    label_gain=[0, 1, 0, 10], # Index 0=0, 1=1, 2=0 (unused), 3=10 (Winner)\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "# C. Random Forest Regressor (The Stabilizer)\n",
    "# Treats ranking as a regression problem (predicting relevance score 0, 1, 3)\n",
    "# Good for small datasets where LTR overfits.\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=500,\n",
    "    max_depth=6,           # Constrained depth\n",
    "    max_features='sqrt',   # Forces diversity\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# --- 2. Validation Loop with Ensemble ---\n",
    "\n",
    "features = [\n",
    "    'prev_discipline_ratio', \n",
    "    'prev_dq_severity', \n",
    "    'prev_PF', \n",
    "    'prev_efficiency', \n",
    "    'prev_minutes', \n",
    "    'years_in_league', \n",
    "    'cumulative_all_stars', \n",
    "    'prev_sp_wins'\n",
    "]\n",
    "\n",
    "all_results = []\n",
    "print(f\"{'='*20} STARTING ENSEMBLE VALIDATION {'='*20}\")\n",
    "\n",
    "for test_year in range(4, 11):\n",
    "    print(f\"\\nProcessing Year {test_year}...\")\n",
    "    \n",
    "    # 1. Split Data\n",
    "    train_df = final_df[final_df['year'] < test_year].copy().sort_values('year')\n",
    "    test_df = final_df[final_df['year'] == test_year].copy().sort_values('year')\n",
    "    \n",
    "    if test_df['is_winner'].max() < 3: continue\n",
    "\n",
    "    train_df = train_df[train_df['prev_minutes'] > 200]\n",
    "    test_df = test_df[test_df['prev_minutes'] > 200]\n",
    "    \n",
    "    # Check if winner survived filter (Safety)\n",
    "    if test_df['is_winner'].max() < 3:\n",
    "        print(\"   ! Winner was filtered out by low minutes. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Prepare Inputs\n",
    "    X_train, y_train = train_df[features], train_df['is_winner']\n",
    "    qid_train = train_df['year']\n",
    "    \n",
    "    X_test, y_test = test_df[features], test_df['is_winner']\n",
    "    qid_test = test_df['year']\n",
    "    \n",
    "    # Group counts for LGBM\n",
    "    group_train = train_df.groupby('year').size().to_list()\n",
    "    group_test = test_df.groupby('year').size().to_list()\n",
    "\n",
    "    # --- 3. Train Individual Models ---\n",
    "    \n",
    "    # Fit XGB\n",
    "    xgb_model.fit(X_train, y_train, qid=qid_train, verbose=False)\n",
    "    pred_xgb = xgb_model.predict(X_test)\n",
    "    \n",
    "    # Fit LGBM\n",
    "    lgb_model.fit(X_train, y_train, group=group_train, eval_group=[group_test], eval_set=[(X_test, y_test)])\n",
    "    pred_lgb = lgb_model.predict(X_test)\n",
    "    \n",
    "    # Fit RF (Pointwise)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "    # --- 4. Ensemble Voting (Min-Max Scaling + Averaging) ---\n",
    "    # We must scale scores because XGB might output 0-10 and RF 0-3\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    p_xgb_norm = scaler.fit_transform(pred_xgb.reshape(-1, 1)).flatten()\n",
    "    p_lgb_norm = scaler.fit_transform(pred_lgb.reshape(-1, 1)).flatten()\n",
    "    p_rf_norm  = scaler.fit_transform(pred_rf.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Weighted Average: Give more weight to Stability (RF) if Rankers are failing\n",
    "    # Weights: XGB(35%), LGB(35%), RF(30%)\n",
    "    final_score = (p_xgb_norm * 0.35) + (p_lgb_norm * 0.35) + (p_rf_norm * 0.30)\n",
    "    \n",
    "    # --- 5. Evaluation ---\n",
    "    temp_res = test_df.copy()\n",
    "    temp_res['score'] = final_score\n",
    "    \n",
    "    ranked = temp_res.sort_values('score', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    try:\n",
    "        winner_rank = ranked[ranked['is_winner'] == 3].index[0] + 1\n",
    "        winner_id = ranked[ranked['is_winner'] == 3]['playerID'].values[0]\n",
    "    except IndexError:\n",
    "        winner_rank = 99\n",
    "        winner_id = \"Unknown\"\n",
    "        \n",
    "    print(f\"   ðŸ† Ensemble Rank: {winner_rank} ({winner_id})\")\n",
    "    \n",
    "    all_results.append({\n",
    "        'Year': test_year,\n",
    "        'Rank': winner_rank,\n",
    "        'MRR': 1/winner_rank\n",
    "    })\n",
    "\n",
    "# --- Summary ---\n",
    "if all_results:\n",
    "    res_df = pd.DataFrame(all_results)\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(f\"Final Avg Rank: {res_df['Rank'].mean():.2f}\")\n",
    "    print(f\"Final MRR: {res_df['MRR'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bc632e",
   "metadata": {},
   "source": [
    "### 5. Validation Loop (Walk-Forward)\n",
    "We validate the ensemble by simulating the voting process for Years 4 through 10.\n",
    "* **Metric**: **Rank of True Winner**. (Did we put the actual winner in the #1 spot?)\n",
    "* **Ensemble Logic**: We normalize the scores from all three models (MinMax scaling) and compute a weighted average:\n",
    "    * 35% XGBoost\n",
    "    * 35% LightGBM\n",
    "    * 30% Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28ef5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Summary & Comparison ---\n",
    "\n",
    "if all_results:\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "\n",
    "    # Since we are using a single Ensemble approach, we summarize the overall performance\n",
    "    # rather than grouping by 'Model'.\n",
    "    summary = {\n",
    "        'Avg Winner Rank': results_df['Rank'].mean(),\n",
    "        'Mean Reciprocal Rank (MRR)': results_df['MRR'].mean()\n",
    "    }\n",
    "\n",
    "    print(\"\\nðŸ† Ensemble Performance Summary ðŸ†\")\n",
    "    print(f\"   Avg Winner Rank: {summary['Avg Winner Rank']:.2f}\")\n",
    "    print(f\"   Mean Reciprocal Rank: {summary['Mean Reciprocal Rank (MRR)']:.4f}\")\n",
    "\n",
    "    # Detailed Year-by-Year\n",
    "    print(f\"\\n--- Year-by-Year Ensemble Performance ---\")\n",
    "    display(results_df[['Year', 'Rank', 'MRR']])\n",
    "else:\n",
    "    print(\"No results generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25366e0a",
   "metadata": {},
   "source": [
    "### 6. Final Prediction for Test Season (Year 11)\n",
    "We now retrain the entire ensemble on the full 10-year history and generate the \"Ballot\" for Year 11.\n",
    "\n",
    "\n",
    "The output below represents the **Top 5 Candidates** most likely to win the Sportsmanship Award based on their behavior in Year 10 and their career reputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022f7332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Final Prediction for Year 11 (Ensemble) ---\n",
    "\n",
    "print(f\"{'='*30} YEAR 11 PREDICTIONS {'='*30}\")\n",
    "\n",
    "# 1. Prepare Training Data (All Years 1-10)\n",
    "# Sort by year is crucial for LTR models\n",
    "train_final = final_df[final_df['year'] < 11].copy().sort_values('year')\n",
    "test_final = final_df[final_df['year'] == 11].copy().sort_values('year')\n",
    "\n",
    "# Apply Elite Filter to Training (Reduce noise)\n",
    "train_final = train_final[train_final['prev_minutes'] > 200]\n",
    "\n",
    "# Prepare Feature Matrices\n",
    "X_train = train_final[features]\n",
    "y_train = train_final['is_winner']\n",
    "qid_train = train_final['year']\n",
    "group_train = train_final.groupby('year').size().to_list()\n",
    "\n",
    "X_test = test_final[features]\n",
    "\n",
    "# 2. Train Individual Models on Full History\n",
    "\n",
    "# A. XGBoost\n",
    "xgb_model.fit(X_train, y_train, qid=qid_train, verbose=False)\n",
    "pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# B. LightGBM (LGBM does not use evaluation sets here, just trains)\n",
    "lgb_model.fit(X_train, y_train, group=group_train)\n",
    "pred_lgb = lgb_model.predict(X_test)\n",
    "\n",
    "# C. Random Forest\n",
    "rf_model.fit(X_train, y_train)\n",
    "pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# 3. Ensemble Voting (Min-Max Scaling + Weighted Average)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "p_xgb_norm = scaler.fit_transform(pred_xgb.reshape(-1, 1)).flatten()\n",
    "p_lgb_norm = scaler.fit_transform(pred_lgb.reshape(-1, 1)).flatten()\n",
    "p_rf_norm  = scaler.fit_transform(pred_rf.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Same weights as validation: XGB(35%), LGB(35%), RF(30%)\n",
    "final_scores = (p_xgb_norm * 0.35) + (p_lgb_norm * 0.35) + (p_rf_norm * 0.30)\n",
    "\n",
    "# 4. Output Top Candidates\n",
    "test_final['pred_score'] = final_scores\n",
    "top_picks = test_final.sort_values('pred_score', ascending=False).head(5)\n",
    "\n",
    "print(\"\\nðŸ† Top 5 Candidates for Year 11:\")\n",
    "for idx, row in top_picks.iterrows():\n",
    "    print(f\"   {idx+1}. {row['playerID']} (Score: {row['pred_score']:.4f})\")\n",
    "    print(f\"       > Prev Disc: {row['prev_discipline_ratio']:.1f} | Prev Eff: {row['prev_efficiency']:.0f} | Prev Wins: {row['prev_sp_wins']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
