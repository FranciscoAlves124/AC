{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0979d9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Define File Paths\n",
    "# Adjust these to match your exact local folder structure\n",
    "initial_path = '../../data/initial_data/' \n",
    "test_path = '../../data/test_data/'\n",
    "\n",
    "def load_and_combine(filename):\n",
    "    \"\"\"\n",
    "    Loads historical data and appends test data if available.\n",
    "    Ensures Year 11 is part of the dataframe for feature generation.\n",
    "    \"\"\"\n",
    "    # Load historical data\n",
    "    df = pd.read_csv(f\"{initial_path}{filename}\")\n",
    "    \n",
    "    # Try to load test data\n",
    "    try:\n",
    "        df_test = pd.read_csv(f\"{test_path}{filename}\")\n",
    "        # Concatenate\n",
    "        df = pd.concat([df, df_test], ignore_index=True)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Note: No test file found for {filename}\")\n",
    "        pass\n",
    "    return df\n",
    "\n",
    "# Load all relevant tables\n",
    "coaches = load_and_combine('coaches.csv')\n",
    "teams = load_and_combine('teams.csv')\n",
    "players_teams = load_and_combine('players_teams.csv')\n",
    "awards = pd.read_csv(f\"{initial_path}awards_players.csv\")\n",
    "\n",
    "# Sort for consistent lag calculation\n",
    "teams = teams.sort_values(['tmID', 'year'])\n",
    "coaches = coaches.sort_values(['tmID', 'year'])\n",
    "\n",
    "print(f\"Teams loaded: {teams.shape}\")\n",
    "print(f\"Coaches loaded: {coaches.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc888b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature Engineering: Team Expectations ---\n",
    "\n",
    "# 1. Calculate derived metrics for TRAINING years (where we have stats)\n",
    "# Pythagorean Expectation Formula for Basketball (exponent ~13.91)\n",
    "# We use a small epsilon to avoid division by zero\n",
    "teams['o_pts'] = teams['o_pts'].fillna(0)\n",
    "teams['d_pts'] = teams['d_pts'].fillna(0)\n",
    "\n",
    "# Real Win %\n",
    "teams['win_pct'] = teams['won'] / (teams['won'] + teams['lost'])\n",
    "\n",
    "# Expected Win % based on points scored/allowed\n",
    "teams['expected_win_pct'] = (teams['o_pts']**13.91) / \\\n",
    "                            ((teams['o_pts']**13.91) + (teams['d_pts']**13.91) + 1e-9)\n",
    "\n",
    "# Luck Factor: Did they win more than they should have?\n",
    "# High luck often implies regression next year\n",
    "# Low luck implies bounce-back.\n",
    "teams['luck_factor'] = teams['win_pct'] - teams['expected_win_pct']\n",
    "\n",
    "# 2. SHIFT metrics to create \"Previous Season\" features\n",
    "# For Year 11, this pulls Year 10 stats into the row.\n",
    "teams['prev_win_pct'] = teams.groupby('tmID')['win_pct'].shift(1)\n",
    "teams['prev_expected_win_pct'] = teams.groupby('tmID')['expected_win_pct'].shift(1)\n",
    "teams['prev_luck_factor'] = teams.groupby('tmID')['luck_factor'].shift(1)\n",
    "\n",
    "# Fill NaNs for the very first season of a team (usually average or 0)\n",
    "teams['prev_win_pct'] = teams['prev_win_pct'].fillna(0.5)\n",
    "teams['prev_expected_win_pct'] = teams['prev_expected_win_pct'].fillna(0.5)\n",
    "teams['prev_luck_factor'] = teams['prev_luck_factor'].fillna(0)\n",
    "\n",
    "# Select only the features we need for the merge later\n",
    "team_features = teams[['year', 'tmID', 'prev_win_pct', 'prev_expected_win_pct', 'prev_luck_factor']].copy()\n",
    "\n",
    "print(\"Team features calculated. Head:\")\n",
    "display(team_features.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d81666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature Engineering: Roster Talent ---\n",
    "\n",
    "# Calculate Player Efficiency in previous years\n",
    "pt_stats = players_teams.copy()\n",
    "cols_to_fix = ['points', 'rebounds', 'assists', 'steals', 'blocks', \n",
    "               'fgAttempted', 'fgMade', 'ftAttempted', 'ftMade', 'turnovers']\n",
    "pt_stats[cols_to_fix] = pt_stats[cols_to_fix].fillna(0)\n",
    "\n",
    "pt_stats['efficiency'] = (\n",
    "    pt_stats['points'] + pt_stats['rebounds'] + pt_stats['assists'] + \n",
    "    pt_stats['steals'] + pt_stats['blocks'] - \n",
    "    (pt_stats['fgAttempted'] - pt_stats['fgMade']) - \n",
    "    (pt_stats['ftAttempted'] - pt_stats['ftMade']) - \n",
    "    pt_stats['turnovers']\n",
    ")\n",
    "\n",
    "# Create a lookup for Player Efficiency in Year Y\n",
    "eff_lookup = pt_stats[['playerID', 'year', 'efficiency']].copy()\n",
    "eff_lookup['next_year'] = eff_lookup['year'] + 1  # This stat applies to the NEXT season\n",
    "eff_lookup = eff_lookup[['playerID', 'next_year', 'efficiency']]\n",
    "\n",
    "# Map efficiency to the Current Roster (Year T)\n",
    "# We join the Roster of Year T with the Efficiency of Year T-1\n",
    "roster = players_teams[['playerID', 'year', 'tmID']]\n",
    "roster_with_talent = roster.merge(\n",
    "    eff_lookup, \n",
    "    left_on=['playerID', 'year'], \n",
    "    right_on=['playerID', 'next_year'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Sum up efficiency per team to get \"Aggregate Talent\"\n",
    "talent_score = roster_with_talent.groupby(['tmID', 'year'])['efficiency'].sum().reset_index()\n",
    "talent_score.rename(columns={'efficiency': 'talent_agg'}, inplace=True)\n",
    "\n",
    "# Normalize and Create \"Turnaround Potential\"\n",
    "feature_df = team_features.merge(talent_score, on=['tmID', 'year'], how='left')\n",
    "\n",
    "# Fill missing talent (e.g. Year 1 or new teams) with mean\n",
    "feature_df['talent_agg'] = feature_df['talent_agg'].fillna(feature_df['talent_agg'].mean())\n",
    "\n",
    "# Z-Score Normalization PER YEAR\n",
    "feature_df['talent_zscore'] = feature_df.groupby('year')['talent_agg'].transform(lambda x: (x - x.mean()) / x.std())\n",
    "feature_df['prev_wins_zscore'] = feature_df.groupby('year')['prev_win_pct'].transform(lambda x: (x - x.mean()) / x.std())\n",
    "\n",
    "feature_df['talent_zscore'] = feature_df['talent_zscore'].fillna(0)\n",
    "feature_df['prev_wins_zscore'] = feature_df['prev_wins_zscore'].fillna(0)\n",
    "\n",
    "# The Narrative Metric: High Talent (Z > 0) + Low Past Wins (Z < 0) = High Potential\n",
    "feature_df['turnaround_potential'] = feature_df['talent_zscore'] - feature_df['prev_wins_zscore']\n",
    "\n",
    "display(feature_df.head())\n",
    "print(\"Talent features ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563b3306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature Engineering: Coach Narrative ---\n",
    "\n",
    "# 1. Identify Main Coach (stint 0 or 1, usually 0 is head coach at start)\n",
    "main_coaches = coaches[(coaches['stint'] == 0)].copy()\n",
    "\n",
    "# 2. Calculate Tenure (How long have they been at THIS team?)\n",
    "# We group by Team and create a cumulative count of years\n",
    "main_coaches['coach_tenure'] = main_coaches.groupby(['tmID', 'coachID']).cumcount() + 1\n",
    "\n",
    "# 3. Calculate Past Awards (Voter Fatigue)\n",
    "# Did this coach win COTY before?\n",
    "coty_history = awards[awards['award'] == 'Coach of the Year'].copy()\n",
    "coty_history['is_coty'] = 1\n",
    "coty_history = coty_history[['playerID', 'year', 'is_coty']].rename(columns={'playerID': 'coachID'})\n",
    "\n",
    "# Cumulative sum of past awards (Shifted, so we only know about *past* awards)\n",
    "# We create a full skeleton of coach/year to avoid missing years breaking the cumsum\n",
    "coach_years = main_coaches[['coachID', 'year']].drop_duplicates()\n",
    "coach_years = coach_years.merge(coty_history, on=['coachID', 'year'], how='left').fillna(0)\n",
    "\n",
    "# Calculate cumulative awards, then shift by 1 to represent \"awards won PRIOR to this season\"\n",
    "coach_years['prev_awards'] = coach_years.groupby('coachID')['is_coty'].cumsum().shift(1).fillna(0)\n",
    "\n",
    "# 4. Merge Coach Features into Main DataFrame\n",
    "final_df = feature_df.merge(main_coaches[['year', 'tmID', 'coachID', 'coach_tenure']], on=['year', 'tmID'], how='inner')\n",
    "final_df = final_df.merge(coach_years[['year', 'coachID', 'prev_awards']], on=['year', 'coachID'], how='left')\n",
    "\n",
    "# 5. Add Target Variable (Current Year COTY)\n",
    "final_df = final_df.merge(coty_history, on=['year', 'coachID'], how='left')\n",
    "final_df['is_coty'] = final_df['is_coty'].fillna(0).astype(int)\n",
    "\n",
    "# Final Cleanup\n",
    "final_df['prev_awards'] = final_df['prev_awards'].fillna(0)\n",
    "\n",
    "print(f\"Final Dataset Shape: {final_df.shape}\")\n",
    "display(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b8c1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# --- 1. Setup Models & Metrics ---\n",
    "\n",
    "features = [\n",
    "    'prev_win_pct', \n",
    "    'prev_expected_win_pct', \n",
    "    'prev_luck_factor', \n",
    "    'talent_zscore', \n",
    "    'turnaround_potential', \n",
    "    'coach_tenure', \n",
    "    'prev_awards'\n",
    "]\n",
    "target = 'is_coty'\n",
    "\n",
    "# Initialize a list to store results for every year and model\n",
    "all_results = []\n",
    "\n",
    "# Define Stacking Estimators\n",
    "estimators = [\n",
    "    ('xgb', XGBClassifier(n_estimators=500, max_depth=4, learning_rate=0.05, \n",
    "                          subsample=0.8, eval_metric='auc', random_state=42)),\n",
    "    ('rf', RandomForestClassifier(n_estimators=500, class_weight='balanced', random_state=42)),\n",
    "    ('svc', SVC(probability=True, kernel='rbf', class_weight='balanced', random_state=42))\n",
    "]\n",
    "\n",
    "# Define the models to compare\n",
    "models = {\n",
    "    'XGBoost': XGBClassifier(\n",
    "        n_estimators=500, max_depth=4, learning_rate=0.05, \n",
    "        subsample=0.8, colsample_bytree=0.8, eval_metric='auc', random_state=42\n",
    "    ),\n",
    "    'LogisticReg': LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42),\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=500, class_weight='balanced', random_state=42),\n",
    "    'GaussianProcess': GaussianProcessClassifier(kernel=1.0 * RBF(1.0, length_scale_bounds=(1e-7, 1e7)), random_state=42),\n",
    "    'StackingEnsemble': StackingClassifier(\n",
    "        estimators=estimators,\n",
    "        final_estimator=LogisticRegression(class_weight='balanced'),\n",
    "        cv=3\n",
    "    )\n",
    "}\n",
    "\n",
    "# --- 2. Walk-Forward Validation Loop ---\n",
    "# We start from year 4 to ensure we have enough training history\n",
    "for test_year in range(4, 11):\n",
    "    print(f\"\\nProcessing Year {test_year}...\")\n",
    "    \n",
    "    # Split Data\n",
    "    train_df = final_df[final_df['year'] < test_year].copy()\n",
    "    test_df = final_df[final_df['year'] == test_year].copy()\n",
    "    \n",
    "    # Skip years with no data or no winner defined\n",
    "    if test_df.empty or test_df['is_coty'].sum() == 0:\n",
    "        continue\n",
    "\n",
    "    X_train = train_df[features]\n",
    "    y_train = train_df[target]\n",
    "    X_test = test_df[features]\n",
    "    y_test = test_df[target]\n",
    "    \n",
    "    # Scale data (Important for Logistic Regression)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Train and Evaluate Each Model\n",
    "    for name, model in models.items():\n",
    "        # Tree models handle unscaled data better/natively\n",
    "        if name in ['XGBoost', 'RandomForest']:\n",
    "            # Dynamic weighting for XGBoost\n",
    "            ratio = (len(y_train) - sum(y_train)) / sum(y_train)\n",
    "            if hasattr(model, 'set_params'):\n",
    "                try:\n",
    "                    model.set_params(scale_pos_weight=ratio)\n",
    "                except:\n",
    "                    pass # RF doesn't use scale_pos_weight\n",
    "            model.fit(X_train, y_train)\n",
    "            probs = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Distance/Probability models need scaling\n",
    "        else:\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            probs = model.predict_proba(X_test_scaled)[:, 1]\n",
    "            \n",
    "        # Store predictions in a temp df for ranking\n",
    "        temp_res = test_df.copy()\n",
    "        temp_res['prob'] = probs\n",
    "        \n",
    "        # --- Calculate Metrics for this Year ---\n",
    "        \n",
    "        # Sort candidates by probability\n",
    "        ranked = temp_res.sort_values('prob', ascending=False).reset_index(drop=True)\n",
    "        \n",
    "        # Find rank of actual winner (Index + 1)\n",
    "        winner_rank = ranked[ranked['is_coty'] == 1].index[0] + 1\n",
    "        \n",
    "        # Metrics\n",
    "        is_top1 = 1 if winner_rank == 1 else 0\n",
    "        is_top3 = 1 if winner_rank <= 3 else 0\n",
    "        mrr = 1 / winner_rank\n",
    "        # Log Loss (for single year)\n",
    "        ll = log_loss(y_test, probs, labels=[0,1])\n",
    "        \n",
    "        all_results.append({\n",
    "            'Year': test_year,\n",
    "            'Model': name,\n",
    "            'Winner_Rank': winner_rank,\n",
    "            'Top1_Acc': is_top1,\n",
    "            'Top3_Acc': is_top3,\n",
    "            'MRR': mrr,\n",
    "            'LogLoss': ll\n",
    "        })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb126bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Final Summary & Comparison ---\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Group by Model to get averages\n",
    "summary = results_df.groupby('Model').agg({\n",
    "    'Winner_Rank': 'mean',\n",
    "    'Top1_Acc': 'mean',\n",
    "    'Top3_Acc': 'mean',\n",
    "    'MRR': 'mean',\n",
    "    'LogLoss': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Rename for presentation\n",
    "summary = summary.rename(columns={\n",
    "    'Winner_Rank': 'Avg Winner Rank',\n",
    "    'Top1_Acc': 'Top-1 Accuracy',\n",
    "    'Top3_Acc': 'Top-3 Accuracy',\n",
    "    'MRR': 'Mean Reciprocal Rank',\n",
    "    'LogLoss': 'Log Loss (Lower is Better)'\n",
    "})\n",
    "\n",
    "print(\"\\nðŸ† Model Comparison Summary ðŸ†\")\n",
    "display(summary.sort_values('Mean Reciprocal Rank', ascending=False).reset_index(drop=True).style\n",
    "    .background_gradient(cmap='YlGn_r', subset=['Avg Winner Rank'], vmin=1, vmax=summary['Avg Winner Rank'].max() if summary['Avg Winner Rank'].max() > 1 else 2) # Assuming 1 is the best rank, higher is worse. Max can be variable.\n",
    "    .background_gradient(cmap='YlGn', subset=['Top-1 Accuracy'], vmin=0, vmax=1)\n",
    "    .background_gradient(cmap='YlGn', subset=['Top-3 Accuracy'], vmin=0, vmax=1)\n",
    "    .background_gradient(cmap='YlGn', subset=['Mean Reciprocal Rank'], vmin=0, vmax=1)\n",
    "    .background_gradient(cmap='YlGn_r', subset=['Log Loss (Lower is Better)'], vmin=0, vmax=summary['Log Loss (Lower is Better)'].max())\n",
    ")\n",
    "\n",
    "for model in models:\n",
    "    print(f\"\\n--- Year-by-Year Performance ({model}) ---\")\n",
    "    display(results_df[results_df['Model'] == model][['Year', 'Winner_Rank', 'Top1_Acc', 'Top3_Acc', 'MRR']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9428c455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. --- Final Prediction for Year 11 ---\n",
    "\n",
    "# Train on ALL history (Years 1-10)\n",
    "train_df_final = final_df[final_df['year'] < 11].copy()\n",
    "test_df_final = final_df[final_df['year'] == 11].copy()\n",
    "\n",
    "X_train_final = train_df_final[features]\n",
    "y_train_final = train_df_final[target]\n",
    "X_test_final = test_df_final[features]\n",
    "\n",
    "# Create Scaled Versions for Non-Tree Models (LogReg, SVC, GaussianProcess)\n",
    "scaler_final = StandardScaler()\n",
    "X_train_scaled = scaler_final.fit_transform(X_train_final)\n",
    "X_test_scaled = scaler_final.transform(X_test_final)\n",
    "\n",
    "print(f\"{'='*30} YEAR 11 PREDICTIONS {'='*30}\")\n",
    "\n",
    "# Iterate, train and predict\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nâš™ï¸  Model: {name}\")\n",
    "    \n",
    "    # Select Data Type based on Model\n",
    "    if name in ['XGBoost', 'RandomForest']:\n",
    "        # Update scale_pos_weight for XGBoost to reflect full dataset imbalance\n",
    "        ratio = (len(y_train_final) - sum(y_train_final)) / sum(y_train_final)\n",
    "        if hasattr(model, 'set_params'):\n",
    "            try:\n",
    "                model.set_params(scale_pos_weight=ratio)\n",
    "            except:\n",
    "                pass \n",
    "        \n",
    "        model.fit(X_train_final, y_train_final)\n",
    "        probs = model.predict_proba(X_test_final)[:, 1]\n",
    "        \n",
    "    else:\n",
    "        # Distance/Linear models use scaled data\n",
    "        model.fit(X_train_scaled, y_train_final)\n",
    "        probs = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    # Identify the Winner\n",
    "    # Assign probabilities to a temp dataframe to find the max\n",
    "    test_df_final['coty_prob'] = probs\n",
    "    top_pick = test_df_final.sort_values('coty_prob', ascending=False).iloc[0]\n",
    "    \n",
    "    # Print Formatted Result\n",
    "    print(f\"   ðŸ† Prediction:  {top_pick['coachID']} ({top_pick['tmID']})\")\n",
    "    print(f\"   ðŸ“ˆ Probability: {top_pick['coty_prob']:.2%}\")\n",
    "    print(f\"   ðŸ“– Narrative:   Turnaround Potential = {top_pick['turnaround_potential']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
