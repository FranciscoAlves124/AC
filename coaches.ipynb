{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d5c37f74",
      "metadata": {},
      "source": [
        "# Coach Change Prediction\n",
        "\n",
        "## 1. Project Context\n",
        "As outlined in the project overview, the goal of this notebook is to address **Core Objective: Coach Changes**. We aim to predict the set of teams that will change coaches at the end of the test season (Year 10).\n",
        "\n",
        "## 2. Methodology\n",
        "To achieve this, we will model the problem as a **Binary Classification** task:\n",
        "* **Target ($y$):** `1` if a team changes its coach between the current season and the next; `0` otherwise.\n",
        "* **Features ($X$):** We will utilize historical basketball data including team performance metrics, coaching history, and player aggregates.\n",
        "\n",
        "We will follow this workflow:\n",
        "1.  **Data Loading:** Ingest relational tables (`teams`, `coaches`, `players`, etc.).\n",
        "2.  **Target Engineering:** Construct the labeled target variable by looking ahead one season.\n",
        "3.  **Feature Engineering:** Aggregate player awards, coach tenure, and derive advanced metrics (Win %, Point Differential).\n",
        "4.  **Modeling:** Train and evaluate multiple classifiers (Random Forest, Gradient Boosting, SVM, etc.).\n",
        "5.  **Prediction:** Generate predictions for the final test season (Year 10)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "initial_id",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-10T17:23:31.823623Z",
          "start_time": "2025-11-10T17:23:31.492311Z"
        },
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "PYDEVD_DISABLE_FILE_VALIDATION=1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dd5e04b",
      "metadata": {},
      "source": [
        "### 2.1 Load Data Sources\n",
        "We utilize the project's relational database consisting of teams, coaches, and player statistics.\n",
        "* **`teams`:** Team performance per season.\n",
        "* **`coaches`:** Records of coaches managing teams.\n",
        "* **`teams_post`:** Post-season results.\n",
        "* **`players_teams`:** Player performance stats.\n",
        "* **`awards_players`:** Awards received by players."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c9f180961aee6c7",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-10T18:14:05.791693Z",
          "start_time": "2025-11-10T18:14:05.779283Z"
        }
      },
      "outputs": [],
      "source": [
        "teams_df = pd.read_csv(\"project_data/initial_data/teams.csv\").sort_values(by=[\"year\", \"tmID\"])\n",
        "coaches_df = pd.read_csv(\"project_data/initial_data/coaches.csv\").sort_values(by=[\"year\", \"tmID\"])\n",
        "teams_post_df = pd.read_csv(\"project_data/initial_data/teams_post.csv\").sort_values(by=[\"year\", \"tmID\"])\n",
        "players_teams_df = pd.read_csv(\"project_data/initial_data/players_teams.csv\").sort_values(by=[\"year\", \"tmID\"])\n",
        "awards_players_df = pd.read_csv(\"project_data/initial_data/awards_players.csv\").sort_values(by=[\"year\"])\n",
        "\n",
        "print(f\"Loaded {len(teams_df)} team-season records.\")\n",
        "display(teams_df.head())\n",
        "\n",
        "print(f\"Loaded {len(coaches_df)} coach records.\")\n",
        "display(coaches_df.head())\n",
        "\n",
        "print(f\"Loaded {len(teams_post_df)} post-season team records.\")\n",
        "display(teams_post_df.head())\n",
        "\n",
        "print(f\"Loaded {len(players_teams_df)} player records.\")\n",
        "display(players_teams_df.head())\n",
        "\n",
        "print(f\"Loaded {len(awards_players_df)} player award records.\")\n",
        "display(awards_players_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0289f278",
      "metadata": {},
      "source": [
        "### 2.2 Target Variable Definition: `CoachChange`\n",
        "We need to define a target variable that indicates if a coach will be replaced in the *middle of the* season.\n",
        "\n",
        "**Logic:**\n",
        "1. TODO: Fix this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ea049ac85765ca6",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-10T18:04:35.545141Z",
          "start_time": "2025-11-10T18:04:35.533033Z"
        }
      },
      "outputs": [],
      "source": [
        "target_df = coaches_df.groupby(['year', 'tmID'])['stint'].max().reset_index()\n",
        "target_df['CoachChange'] = target_df['stint'] > 0\n",
        "\n",
        "target_df = target_df[['year', 'tmID', 'CoachChange']]\n",
        "\n",
        "print(\"\\n--- Target Variable 'CoachChange' Created ---\")\n",
        "print(target_df[target_df['CoachChange'] == True])\n",
        "print(f\"\\nTotal 'CoachChange = True' events: {int(target_df['CoachChange'].sum())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39205ca0",
      "metadata": {},
      "source": [
        "### 3. Feature Engineering & Data Assembly\n",
        "To improve prediction accuracy, we need to move beyond raw stats. We will engineer features that reflect the \"status\" of the team and the coach:\n",
        "\n",
        "1.  **Player Talent:** We quantify the talent level by counting the number of individual awards (`awards_players`) a team's players won in a given season.\n",
        "2.  **Coach History:** We calculate `stint_max` (how long the coach has been there) and their historical post-season success (`coach_post_wins`).\n",
        "3.  **Data Merge:** We aggregate these features into a single analytical table (`final_df`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93480272b129245b",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-10T18:38:54.654974Z",
          "start_time": "2025-11-10T18:38:54.632099Z"
        }
      },
      "outputs": [],
      "source": [
        "# --- 3. Engineer and Merge Feature Sets ---\n",
        "\n",
        "# Player Awards\n",
        "stats_single_team = (players_teams_df\n",
        "                     .sort_values(['playerID', 'year', 'stint'])\n",
        "                     .drop_duplicates(subset=['playerID', 'year'], keep='last')\n",
        "                     [['playerID', 'year', 'tmID']]\n",
        ")\n",
        "\n",
        "awards_with_team_df = (\n",
        "    awards_players_df\n",
        "    .merge(stats_single_team, on=['playerID', 'year'])\n",
        ")\n",
        "\n",
        "awards_count_df = (\n",
        "    awards_with_team_df\n",
        "    .groupby(['tmID', 'year'])\n",
        "    .size()\n",
        "    .reset_index(name='num_player_awards')\n",
        ")\n",
        "\n",
        "# Teams Post\n",
        "teams_post_features = teams_post_df[['tmID', 'year', 'W', 'L']].rename(columns={\n",
        "    'W': 'team_post_W',\n",
        "    'L': 'team_post_L'\n",
        "})\n",
        "\n",
        "# Coach Tenure\n",
        "coaches_df = coaches_df.sort_values(['coachID', 'tmID', 'year'])\n",
        "coaches_df['coach_tenure'] = coaches_df.groupby(['coachID', 'tmID']).cumcount() + 1\n",
        "\n",
        "coach_tenure_df = coaches_df.groupby(['tmID', 'year'])['coach_tenure'].max().reset_index()\n",
        "\n",
        "# Final DataFrame\n",
        "final_df = teams_df.copy()\n",
        "\n",
        "final_df = pd.merge(final_df, target_df, on=['tmID', 'year'], how='left')\n",
        "final_df = pd.merge(final_df, teams_post_features, on=['tmID', 'year'], how='left')\n",
        "final_df = pd.merge(final_df, awards_count_df, on=['tmID', 'year'], how='left')\n",
        "final_df = pd.merge(final_df, coach_tenure_df, on=['tmID', 'year'], how='left')\n",
        "\n",
        "# Clean-up\n",
        "fill_zero_cols = [\n",
        "    'coach_post_wins', 'coach_post_losses', 'team_post_W',\n",
        "    'team_post_L', 'num_player_awards'\n",
        "]\n",
        "\n",
        "for col in fill_zero_cols:\n",
        "    if col in final_df.columns:\n",
        "        final_df[col] = final_df[col].fillna(0)\n",
        "\n",
        "final_df['coach_tenure'] = final_df['coach_tenure'].fillna(1)\n",
        "\n",
        "print(\"\\n--- Final Assembled DataFrame ---\")\n",
        "print(final_df.head())\n",
        "print(f\"\\nFinal DataFrame shape: {final_df.shape}\")\n",
        "print(f\"Columns: {final_df.columns.to_list()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a67c49a",
      "metadata": {},
      "source": [
        "### 3.1 Derived Performance Metrics\n",
        "We convert raw counts into standardized ratios to compare teams across different numbers of games played (`GP`):\n",
        "* **Win Percentage:** $\\frac{Wins}{Wins + Losses}$\n",
        "* **Point Differential:** `o_pts` (Offensive Points) - `d_pts` (Defensive Points).\n",
        "* **Playoff Flag:** A binary indicator of whether the team qualified for the post-season."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc759baa061c43fd",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-10T19:00:03.388995Z",
          "start_time": "2025-11-10T19:00:03.380623Z"
        }
      },
      "outputs": [],
      "source": [
        "# 1. Win Pct & Diff (Standard)\n",
        "final_df['win_pct'] = final_df['won'] / (final_df['won'] + final_df['lost'] + 1e-6)\n",
        "final_df['pt_diff'] = final_df['o_pts'] - final_df['d_pts']\n",
        "\n",
        "# 2. NEW: Year-over-Year Performance Drop (\"The Cliff\")\n",
        "# Sort to ensure shift works correctly\n",
        "final_df = final_df.sort_values(['tmID', 'year'])\n",
        "final_df['prev_win_pct'] = final_df.groupby('tmID')['win_pct'].shift(1)\n",
        "final_df['prev_win_pct'] = final_df['prev_win_pct'].fillna(0.5) # Fill first year with average\n",
        "final_df['win_pct_change'] = final_df['win_pct'] - final_df['prev_win_pct']\n",
        "\n",
        "# 3. NEW: Talent Mismatch (\"Underachiever\")\n",
        "# High talent (awards) but low wins = High Pressure\n",
        "# We add 0.2 to win_pct to avoid exploding numbers for very bad teams\n",
        "final_df['talent_mismatch'] = final_df['num_player_awards'] / (final_df['win_pct'] + 0.2)\n",
        "\n",
        "print(\"--- DataFrame with Engineered Features (sample) ---\")\n",
        "print(final_df[['tmID', 'year', 'win_pct', 'pt_diff', 'win_pct_change', 'talent_mismatch']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81be6a599e8948b3",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-10T19:01:49.447295Z",
          "start_time": "2025-11-10T19:01:49.437419Z"
        }
      },
      "outputs": [],
      "source": [
        "# Define target and features\n",
        "train_df = final_df.dropna(subset=['CoachChange'])\n",
        "y = train_df['CoachChange']\n",
        "\n",
        "# Define features to drop\n",
        "features_to_drop = [\n",
        "    # Targets/IDs\n",
        "    'CoachChange', 'tmID', 'lgID', 'franchID', 'confID', 'divID', 'name', 'arena', 'coachID',\n",
        "    \n",
        "    # Raw stats replaced by derived metrics\n",
        "    'year', 'won', 'lost', 'homeW', 'homeL', 'awayW', 'awayL', 'confW', 'confL', 'conf_win_pct',\n",
        "    'o_pts', 'd_pts', 'pt_diff',\n",
        "    'o_reb', 'd_reb', 'tmTRB', 'opptmTRB', 'tmORB', 'tmDRB', 'opptmORB', 'opptmDRB',\n",
        "    'o_fta', 'o_3pa', 'd_fta', 'd_3pa', 'o_fgm', 'o_ftm', 'o_3pm', 'd_fgm', 'd_ftm', 'd_3pm',\n",
        "    'min', 'made_playoffs', 'team_post_W', 'team_post_L', 'seeded',\n",
        "\n",
        "    'o_fga', 'o_oreb', 'o_dreb', 'o_asts', 'o_pf', 'o_stl', 'o_to', 'o_blk',\n",
        "    'd_fga', 'd_oreb', 'd_dreb', 'd_asts', 'd_pf', 'd_stl', 'd_to', 'd_blk',\n",
        "    \n",
        "    # -- Removed after Feature Correlation Matrix\n",
        "    'firstRound', 'semis', 'finals',\n",
        "]\n",
        "\n",
        "# 3. Create the features DataFrames\n",
        "X = train_df.drop(columns=features_to_drop, errors='ignore')\n",
        "\n",
        "# Convert Y/N columns to 1/0\n",
        "cols_to_map = ['playoff', 'firstRound', 'semis', 'finals']\n",
        "for col in cols_to_map:\n",
        "    if col in X.columns:\n",
        "        X[col] = X[col].map({'Y': 1, 'N': 0})\n",
        "\n",
        "# 4. Handle Missing Values\n",
        "X = X.fillna(0)\n",
        "\n",
        "print(f\"--- Final 'X' (Features) DataFrame ---\")\n",
        "print(f\"Shape of X: {X.shape}\")\n",
        "print(f\"Features: {X.columns.to_list()}\\n\")\n",
        "print(f\"Count of {train_df['CoachChange'].value_counts()}\\n\")\n",
        "\n",
        "print(f\"--- Final 'y' (Target) Series ---\")\n",
        "print(f\"Shape of y: {y.shape}\")\n",
        "print(f\"Class distribution:\\n{y.value_counts()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70f1c1f1",
      "metadata": {},
      "source": [
        "### 5. Exploratory Data Analysis (Correlation)\n",
        "We check for multicollinearity among features. High correlation (e.g., > 0.9) between features can confuse linear models and inflate feature importance in tree models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89df1b53aaa8601c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-10T19:01:53.150838Z",
          "start_time": "2025-11-10T19:01:53.022654Z"
        }
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- C. Check Correlation ---\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "corr_matrix = X.corr().abs()\n",
        "\n",
        "# Create a heatmap\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(corr_matrix, annot=False, cmap='Blues', fmt='.1f')\n",
        "plt.title('Feature Correlation Matrix')\n",
        "plt.show()\n",
        "\n",
        "# You can also manually find high-correlation pairs\n",
        "# Select upper triangle of correlation matrix\n",
        "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "\n",
        "# Find features with correlation greater than 0.9\n",
        "high_corr_features = [column for column in upper_tri.columns if any(upper_tri[column] > 0.9)]\n",
        "\n",
        "if high_corr_features:\n",
        "    print(f\"\\nWARNING!: High Correlation remaining in features: {high_corr_features}\")\n",
        "    print(\"Consider dropping one feature from each correlated pair.\")\n",
        "else:\n",
        "    print(\"\\nNo highly correlated (r > 0.9) features found. Ready for modeling.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcdcf3f5",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer, classification_report\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "def print_metrics(y_true, y_pred, model_name):\n",
        "    \"\"\"Prints common classification metrics.\"\"\"\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
        "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "    \n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "    print(f\"Precision: {prec:.4f}\")\n",
        "    print(f\"Recall: {rec:.4f}\")\n",
        "    print(f\"F1-Score: {f1:.4f}\")\n",
        "    return {\"Model\": model_name, \"Accuracy\": acc, \"Precision\": prec, \"Recall\": rec, \"F1\": f1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c569bd6d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "class YearlyWalkForwardSplit:\n",
        "    \"\"\"\n",
        "    Perform walk-forward validation based on an external year series (list/array/column).\n",
        "    \n",
        "    Train: All years prior to the current test year.\n",
        "    Test:  The specific current test year.\n",
        "    \"\"\"\n",
        "    def __init__(self, year_series):\n",
        "        self.year_series = np.array(year_series)\n",
        "        self.unique_years = np.sort(np.unique(self.year_series))\n",
        "        \n",
        "    def get_n_splits(self, X=None, y=None, groups=None):\n",
        "        return len(self.unique_years) - 1\n",
        "\n",
        "    def split(self, X, y=None, groups=None):\n",
        "        if len(X) != len(self.year_series):\n",
        "            raise ValueError(f\"Data length mismatch! X has {len(X)} rows, but year_series has {len(self.year_series)}.\")\n",
        "\n",
        "        for i in range(1, len(self.unique_years)):\n",
        "            test_year = self.unique_years[i]\n",
        "            \n",
        "            # Train on everything strictly BEFORE the test year\n",
        "            train_mask = self.year_series < test_year\n",
        "            \n",
        "            # Test on ONLY the current test year\n",
        "            test_mask = self.year_series == test_year\n",
        "            \n",
        "            train_indices = np.flatnonzero(train_mask)\n",
        "            test_indices = np.flatnonzero(test_mask)\n",
        "            \n",
        "            yield train_indices, test_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64e6decf",
      "metadata": {},
      "outputs": [],
      "source": [
        "test_year = 10\n",
        "starting_year = 3\n",
        "random_state = 45\n",
        "\n",
        "X_train = X[train_df['year'] < test_year]\n",
        "y_train = y[train_df['year'] < test_year]\n",
        "\n",
        "X_test = X[train_df['year'] == test_year]\n",
        "y_test = y[train_df['year'] == test_year]\n",
        "\n",
        "walk_forward_cv = YearlyWalkForwardSplit(train_df[train_df['year'] < test_year]['year'])\n",
        "f1_scorer = make_scorer(f1_score, pos_label=1, zero_division=0) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed36369a",
      "metadata": {},
      "outputs": [],
      "source": [
        "lr = LogisticRegression(solver='liblinear', max_iter=1000, random_state=42)\n",
        "\n",
        "lr_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('lr', lr)\n",
        "])\n",
        "\n",
        "lr_params = {\n",
        "    'lr__C': [0.01, 0.1, 1, 10, 100],\n",
        "    'lr__penalty': ['l1', 'l2'],\n",
        "    'lr__class_weight': [None, 'balanced']\n",
        "}\n",
        "\n",
        "lr_grid = GridSearchCV(\n",
        "    estimator=lr_pipeline,\n",
        "    param_grid=lr_params,\n",
        "    scoring=f1_scorer,\n",
        "    cv=walk_forward_cv,\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "lr_grid.fit(X_train, y_train)\n",
        "\n",
        "# 5. Get Results\n",
        "print(f\"Best Hyperparameters: {lr_grid.best_params_}\")\n",
        "print(f\"Best Cross-Validated F1 Score: {lr_grid.best_score_:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "706c199f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "xgb_params = {\n",
        "  'n_estimators' : [100, 200, 500],\n",
        "  'learning_rate' : [0.01, 0.05, 0.1],\n",
        "  'max_depth' : [3, 4, 5, 6],\n",
        "  'subsample' : [0.6, 0.8, 1.0],\n",
        "  'scale_pos_weight' : [1, 10, 25],\n",
        "}\n",
        "\n",
        "xgb_classifier = XGBClassifier(random_state=42)\n",
        "\n",
        "xgb_grid = GridSearchCV(\n",
        "    estimator=xgb_classifier,\n",
        "    param_grid=xgb_params,\n",
        "    scoring=f1_scorer,\n",
        "    cv=walk_forward_cv,\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "xgb_grid.fit(\n",
        "  X_train,\n",
        "  y_train\n",
        ")\n",
        "\n",
        "print(f\"Best Hyperparameters: {xgb_grid.best_params_}\")\n",
        "print(f\"Best Cross-Validated F1 Score: {xgb_grid.best_score_:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7fd4d5c",
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_weights = compute_sample_weight(class_weight='balanced', y=y_train)\n",
        "\n",
        "mlp_params = {\n",
        "  'mlp__hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],\n",
        "  'mlp__activation': ['relu', 'tanh'],\n",
        "  'mlp__alpha': [0.0001, 0.001, 0.01],\n",
        "  'mlp__learning_rate_init': [0.001, 0.01],\n",
        "  'mlp__max_iter': [2000],\n",
        "}\n",
        "\n",
        "mlp_pipeline = Pipeline([\n",
        "  ('scaler', StandardScaler()),\n",
        "  ('mlp', MLPClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "mlp_grid = GridSearchCV(\n",
        "  mlp_pipeline, \n",
        "  mlp_params,\n",
        "  cv=walk_forward_cv, \n",
        "  scoring='f1', \n",
        "  verbose=1,\n",
        "  n_jobs=-1,\n",
        ")\n",
        "\n",
        "mlp_grid.fit(\n",
        "  X_train, \n",
        "  y_train,\n",
        "  mlp__sample_weight=sample_weights\n",
        ")\n",
        "\n",
        "print(f\"Best Hyperparameters: {mlp_grid.best_params_}\")\n",
        "print(f\"Best Cross-Validated F1 Score: {mlp_grid.best_score_:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53af1546",
      "metadata": {},
      "outputs": [],
      "source": [
        "best_estimators = {'Logistic Regression': lr_grid.best_estimator_, 'XGBClassifier': xgb_grid.best_estimator_, 'MLP': mlp_grid.best_estimator_}\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, precision_recall_curve, auc\n",
        "\n",
        "def get_metrics_dict(y_true, y_pred, model_name, threshold=None, y_proba=None):\n",
        "    metrics = {\n",
        "        'Model': model_name,\n",
        "        'Threshold': threshold if threshold is not None else 'Default',\n",
        "        'Accuracy': accuracy_score(y_true, y_pred),\n",
        "        'Precision': precision_score(y_true, y_pred, zero_division=0),\n",
        "        'Recall': recall_score(y_true, y_pred, zero_division=0),\n",
        "        'F1-Score': f1_score(y_true, y_pred, zero_division=0),\n",
        "    }\n",
        "    if y_proba is not None:\n",
        "        try:\n",
        "            metrics['ROC AUC'] = roc_auc_score(y_true, y_proba)\n",
        "        except ValueError:\n",
        "            metrics['ROC AUC'] = None\n",
        "        \n",
        "        # Calculate PR AUC\n",
        "        precision, recall, _ = precision_recall_curve(y_true, y_proba)\n",
        "        metrics['PR AUC'] = auc(recall, precision)\n",
        "\n",
        "    return metrics\n",
        "\n",
        "all_metrics = []\n",
        "\n",
        "for name, estimator in best_estimators.items():\n",
        "    if hasattr(estimator, 'predict_proba'):\n",
        "        y_proba = estimator.predict_proba(X_test)[:, 1]\n",
        "        thresholds = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
        "        for threshold in thresholds:\n",
        "            y_pred_threshold = (y_proba >= threshold).astype(int)\n",
        "            metrics = get_metrics_dict(y_test, y_pred_threshold, name, threshold, y_proba)\n",
        "            all_metrics.append(metrics)\n",
        "    else:\n",
        "        # Fallback for estimators that don't have predict_proba (e.g., some SVMs)\n",
        "        y_pred = estimator.predict(X_test)\n",
        "        metrics = get_metrics_dict(y_test, y_pred, name)\n",
        "        all_metrics.append(metrics)\n",
        "\n",
        "metrics_df = pd.DataFrame(all_metrics)\n",
        "\n",
        "# Apply the styling to the DataFrame, focusing on the score columns\n",
        "styled_metrics_df = metrics_df.style.background_gradient(cmap='Greens', subset=['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC AUC', 'PR AUC'])\n",
        "styled_metrics_df"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
