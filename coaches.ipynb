{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d5c37f74",
      "metadata": {},
      "source": [
        "# Coach Change Prediction\n",
        "\n",
        "## 1. Project Context\n",
        "As outlined in the project overview, the goal of this notebook is to address **Core Objective: Coach Changes**. We aim to predict the set of teams that will change coaches at the end of the test season (Year 10).\n",
        "\n",
        "## 2. Methodology\n",
        "To achieve this, we will model the problem as a **Binary Classification** task:\n",
        "* **Target ($y$):** `1` if a team changes its coach between the current season and the next; `0` otherwise.\n",
        "* **Features ($X$):** We will utilize historical basketball data including team performance metrics, coaching history, and player aggregates.\n",
        "\n",
        "We will follow this workflow:\n",
        "1.  **Data Loading:** Ingest relational tables (`teams`, `coaches`, `players`, etc.).\n",
        "2.  **Target Engineering:** Construct the labeled target variable by looking ahead one season.\n",
        "3.  **Feature Engineering:** Aggregate player awards, coach tenure, and derive advanced metrics (Win %, Point Differential).\n",
        "4.  **Modeling:** Train and evaluate multiple classifiers (Random Forest, Gradient Boosting, SVM, etc.).\n",
        "5.  **Prediction:** Generate predictions for the final test season (Year 10)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "initial_id",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-10T17:23:31.823623Z",
          "start_time": "2025-11-10T17:23:31.492311Z"
        },
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "PYDEVD_DISABLE_FILE_VALIDATION=1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dd5e04b",
      "metadata": {},
      "source": [
        "### 2.1 Load Data Sources\n",
        "We utilize the project's relational database consisting of teams, coaches, and player statistics.\n",
        "* **`teams`:** Team performance per season.\n",
        "* **`coaches`:** Records of coaches managing teams.\n",
        "* **`teams_post`:** Post-season results.\n",
        "* **`players_teams`:** Player performance stats.\n",
        "* **`awards_players`:** Awards received by players."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c9f180961aee6c7",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-10T18:14:05.791693Z",
          "start_time": "2025-11-10T18:14:05.779283Z"
        }
      },
      "outputs": [],
      "source": [
        "teams_df = pd.read_csv(\"project_data/initial_data/teams.csv\")\n",
        "coaches_df = pd.read_csv(\"project_data/initial_data/coaches.csv\")\n",
        "teams_post_df = pd.read_csv(\"project_data/initial_data/teams_post.csv\")\n",
        "players_teams_df = pd.read_csv(\"project_data/initial_data/players_teams.csv\")\n",
        "awards_players_df = pd.read_csv(\"project_data/initial_data/awards_players.csv\")\n",
        "\n",
        "print(f\"Loaded {len(teams_df)} team-season records.\")\n",
        "print(f\"Loaded {len(coaches_df)} coach records.\")\n",
        "print(f\"Loaded {len(teams_post_df)} post-season team records.\")\n",
        "print(f\"Loaded {len(players_teams_df)} player records.\")\n",
        "print(f\"Loaded {len(awards_players_df)} player award records.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0289f278",
      "metadata": {},
      "source": [
        "### 2.2 Target Variable Definition: `CoachChange`\n",
        "We need to define a target variable that indicates if a coach will be replaced in the *following* season.\n",
        "\n",
        "**Logic:**\n",
        "1.  We group data by Team (`tmID`).\n",
        "2.  We look at the `coachID` for the *next* year ($t+1$).\n",
        "3.  If `coachID` at $t$ $\\neq$ `coachID` at $t+1$, then `CoachChange = 1`.\n",
        "4.  If the team folds (no record next year), we mark it for removal to avoid training noise.\n",
        "5.  **Test Set:** The final year (Year 10) will have `NaN` as the target, as these are the values we must predict."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ea049ac85765ca6",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-10T18:04:35.545141Z",
          "start_time": "2025-11-10T18:04:35.533033Z"
        }
      },
      "outputs": [],
      "source": [
        "coaches_labeled_df = coaches_df.copy()\n",
        "\n",
        "# Keep only the last coach of each year\n",
        "coaches_labeled_df = coaches_labeled_df.sort_values(by=['tmID', 'year'])\n",
        "\n",
        "# Get the coachID from the next season\n",
        "coaches_labeled_df['NextYearCoachID'] = coaches_labeled_df.groupby('tmID')['coachID'].shift(-1)\n",
        "\n",
        "max_year = coaches_labeled_df['year'].max() \n",
        "\n",
        "conditions = [\n",
        "    # Final Year\n",
        "    (coaches_labeled_df['year'] == max_year),\n",
        "    \n",
        "    # Team folded\n",
        "    (coaches_labeled_df['NextYearCoachID'].isnull()), \n",
        "    \n",
        "    # Same Coach\n",
        "    (coaches_labeled_df['coachID'] == coaches_labeled_df['NextYearCoachID']),\n",
        "    \n",
        "    # Different Coach\n",
        "    (coaches_labeled_df['coachID'] != coaches_labeled_df['NextYearCoachID'])\n",
        "]\n",
        "\n",
        "choices = [\n",
        "    np.nan, # Keep as NaN (Test Set)\n",
        "    -1,     # Mark for Removal (Folded Team)\n",
        "    0,      # No Change\n",
        "    1       # Change\n",
        "]\n",
        "\n",
        "coaches_labeled_df['CoachChange'] = np.select(conditions, choices)\n",
        "coaches_labeled_df = coaches_labeled_df[coaches_labeled_df['CoachChange'] != -1]\n",
        "\n",
        "coach_labels_df = coaches_labeled_df[['tmID', 'year', 'CoachChange']]\n",
        "\n",
        "print(\"\\n--- Target Variable 'CoachChange' Created ---\")\n",
        "print(coach_labels_df[coach_labels_df['CoachChange'] == np.nan])\n",
        "print(f\"\\nTotal 'CoachChange=1' events: {int(coach_labels_df['CoachChange'].sum())}\")\n",
        "print(f\"Total 'CoachChange=np.nan' events: {int(coach_labels_df['CoachChange'].isnull().sum())}\")\n",
        "print(f\"Total entries in max_year({max_year}): {(coach_labels_df['year'] == max_year).sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39205ca0",
      "metadata": {},
      "source": [
        "### 3. Feature Engineering & Data Assembly\n",
        "To improve prediction accuracy, we need to move beyond raw stats. We will engineer features that reflect the \"status\" of the team and the coach:\n",
        "\n",
        "1.  **Player Talent:** We quantify the talent level by counting the number of individual awards (`awards_players`) a team's players won in a given season.\n",
        "2.  **Coach History:** We calculate `stint_max` (how long the coach has been there) and their historical post-season success (`coach_post_wins`).\n",
        "3.  **Data Merge:** We aggregate these features into a single analytical table (`final_df`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93480272b129245b",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-10T18:38:54.654974Z",
          "start_time": "2025-11-10T18:38:54.632099Z"
        }
      },
      "outputs": [],
      "source": [
        "# --- 3. Engineer and Merge Feature Sets ---\n",
        "stats_single_team = (players_teams_df\n",
        "                     .sort_values(['playerID', 'year', 'stint'])\n",
        "                     .drop_duplicates(subset=['playerID', 'year'], keep='last')\n",
        "                     [['playerID', 'year', 'tmID']]\n",
        ")\n",
        "\n",
        "awards_with_team_df = (\n",
        "    awards_players_df\n",
        "    .merge(stats_single_team, on=['playerID', 'year'])\n",
        ")\n",
        "\n",
        "# 3a. Create 'num_player_awards'\n",
        "awards_count_df = (\n",
        "    awards_with_team_df\n",
        "    .groupby(['tmID', 'year'])\n",
        "    .size()\n",
        "    .reset_index(name='num_player_awards')\n",
        ")\n",
        "\n",
        "# 3b. A team might have multiple coaches (stints). We'll sum their post-season stats and take the max stint number.\n",
        "coach_features_agg = coaches_df.groupby(['tmID', 'year']).agg(\n",
        "    stint_max=('stint', 'max'),\n",
        "    coach_post_wins=('post_wins', 'sum'),\n",
        "    coach_post_losses=('post_losses', 'sum')\n",
        ").reset_index()\n",
        "\n",
        "# 3c. Get team-specific post-season features\n",
        "teams_post_features = teams_post_df[['tmID', 'year', 'W', 'L']].rename(columns={\n",
        "    'W': 'team_post_W',\n",
        "    'L': 'team_post_L'\n",
        "})\n",
        "\n",
        "# 3d. Assemble the final DataFrame\n",
        "# Start with the base team stats\n",
        "final_df = teams_df.copy()\n",
        "\n",
        "# Merge the target variable (CoachChange)\n",
        "final_df = pd.merge(final_df, coach_labels_df, on=['tmID', 'year'], how='left')\n",
        "\n",
        "# Merge the aggregated coach features\n",
        "final_df = pd.merge(final_df, coach_features_agg, on=['tmID', 'year'], how='left')\n",
        "\n",
        "# Merge the team post-season features\n",
        "final_df = pd.merge(final_df, teams_post_features, on=['tmID', 'year'], how='left')\n",
        "\n",
        "# Merge the player award count\n",
        "final_df = pd.merge(final_df, awards_count_df, on=['tmID', 'year'], how='left')\n",
        "\n",
        "# --- 4. Clean Up Merged Data ---\n",
        "# Merges create 'NaN' for teams that didn't make playoffs or win awards.\n",
        "# We'll fill these with '0' as 'NaN' means 'zero wins' or 'zero awards'.\n",
        "fill_zero_cols = [\n",
        "    'coach_post_wins', 'coach_post_losses', 'team_post_W',\n",
        "    'team_post_L', 'num_player_awards'\n",
        "]\n",
        "for col in fill_zero_cols:\n",
        "    if col in final_df.columns:\n",
        "        final_df[col] = final_df[col].fillna(0)\n",
        "\n",
        "print(\"\\n--- Final Assembled DataFrame ---\")\n",
        "print(final_df.head())\n",
        "print(f\"\\nFinal DataFrame shape: {final_df.shape}\")\n",
        "print(f\"Columns: {final_df.columns.to_list()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a67c49a",
      "metadata": {},
      "source": [
        "### 3.1 Derived Performance Metrics\n",
        "We convert raw counts into standardized ratios to compare teams across different numbers of games played (`GP`):\n",
        "* **Win Percentage:** $\\frac{Wins}{Wins + Losses}$\n",
        "* **Point Differential:** `o_pts` (Offensive Points) - `d_pts` (Defensive Points).\n",
        "* **Playoff Flag:** A binary indicator of whether the team qualified for the post-season."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc759baa061c43fd",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-10T19:00:03.388995Z",
          "start_time": "2025-11-10T19:00:03.380623Z"
        }
      },
      "outputs": [],
      "source": [
        "# 1. Create Win Percentage\n",
        "# We add a small number (1e-6) to avoid division by zero for any (unlikely) 0-game seasons\n",
        "final_df['win_pct'] = final_df['won'] / (final_df['won'] + final_df['lost'] + 1e-6)\n",
        "final_df['home_win_pct'] = final_df['homeW'] / (final_df['homeW'] + final_df['homeL'] + 1e-6)\n",
        "final_df['conf_win_pct'] = final_df['confW'] / (final_df['confW'] + final_df['confL'] + 1e-6)\n",
        "\n",
        "# 2. Create Point Differential\n",
        "final_df['pt_diff'] = final_df['o_pts'] - final_df['d_pts']\n",
        "\n",
        "# 3. Create a simple 'made_playoffs' flag\n",
        "# If a team has any team_post_W or team_post_L, they were in the post-season.\n",
        "final_df['made_playoffs'] = ((final_df['team_post_W'] > 0) | (final_df['team_post_L'] > 0)).astype(int)\n",
        "\n",
        "print(\"--- DataFrame with Engineered Features (sample) ---\")\n",
        "print(final_df[['tmID', 'year', 'win_pct', 'pt_diff', 'made_playoffs', 'CoachChange']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eff3fe93",
      "metadata": {},
      "source": [
        "### 4. Data Preparation\n",
        "Before modeling, we must clean the feature set to prevent **Data Leakage** and remove redundancy.\n",
        "\n",
        "**Preprocessing Steps:**\n",
        "1.  **Drop Leakage/IDs:** Remove identifiers (`tmID`, `name`) and columns that directly reveal the target.\n",
        "2.  **Remove Redundancy:** Drop raw counters (like `won`, `lost`) in favor of the calculated percentages. Remove highly correlated stat components (e.g., keeping `fga` but dropping `fgm` as they track closely).\n",
        "3.  **Split:** Separate the dataset into:\n",
        "    * **`X` & `y`:** Historical data (Years 1-9) for training/validation.\n",
        "    * **`X_test_predict`:** The final year (Year 10) for the submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81be6a599e8948b3",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-10T19:01:49.447295Z",
          "start_time": "2025-11-10T19:01:49.437419Z"
        }
      },
      "outputs": [],
      "source": [
        "# 1. Separate the target 'y' and the 'test_df'\n",
        "train_df = final_df.dropna(subset=['CoachChange'])\n",
        "y = train_df['CoachChange']\n",
        "\n",
        "test_df = final_df[final_df['CoachChange'].isnull()]\n",
        "test_df = test_df.drop_duplicates(subset=['tmID', 'year'], keep='last')\n",
        "\n",
        "# --- Team IDs for the Final Submission ---\n",
        "teams_test_final = test_df[['tmID', 'year']].reset_index(drop=True)\n",
        "\n",
        "# 2. Define features to drop\n",
        "features_to_drop = [\n",
        "    # --- Target/Leakage ---\n",
        "    'CoachChange',\n",
        "\n",
        "    # --- ID / Text Columns ---\n",
        "    'tmID', 'lgID', 'franchID', 'confID', 'divID', 'name', 'arena',\n",
        "\n",
        "    # --- Replaced by win_pct ---\n",
        "    'won', 'lost', 'homeW', 'homeL', 'awayW', 'awayL', 'confW', 'confL',\n",
        "    'conf_win_pct', # Correlated with win_pct\n",
        "\n",
        "    # --- Replaced by pt_diff (which we ALSO drop) ---\n",
        "    'o_pts', 'd_pts',\n",
        "    'pt_diff', # Correlated with win_pct\n",
        "\n",
        "    # --- 100% Redundant Sums (from teams table) ---\n",
        "    'o_reb', 'd_reb', 'tmTRB', 'opptmTRB',\n",
        "    'tmORB', 'tmDRB', 'opptmORB', 'opptmDRB', # Keep o_oreb, o_dreb, etc.\n",
        "\n",
        "    # --- Redundant Components ---\n",
        "    'o_fta', 'o_3pa', 'd_fta', 'd_3pa', # Keep 'o_fga' and 'd_fga' as primary\n",
        "    'o_fgm', 'o_ftm', 'o_3pm', # Drop 'made' stats, keep 'attempted'\n",
        "    'd_fgm', 'd_ftm', 'd_3pm',\n",
        "\n",
        "    # --- Other Redundancies ---\n",
        "    'min',               # Correlated with GP\n",
        "    'made_playoffs',     # Redundant with 'playoff' column\n",
        "    'team_post_W',       # Redundant with coach_post_wins\n",
        "    'team_post_L',       # Redundant with coach_post_losses\n",
        "    'seeded'             # Redundant with rank\n",
        "]\n",
        "\n",
        "# 3. Create the features DataFrames\n",
        "X = train_df.drop(columns=features_to_drop, errors='ignore')\n",
        "X_test_predict = test_df.drop(columns=features_to_drop, errors='ignore')\n",
        "\n",
        "# Convert Y/N columns to 1/0\n",
        "cols_to_map = ['playoff', 'firstRound', 'semis', 'finals']\n",
        "for col in cols_to_map:\n",
        "    if col in X.columns:\n",
        "        X[col] = X[col].map({'Y': 1, 'N': 0})\n",
        "        X_test_predict[col] = X_test_predict[col].map({'Y': 1, 'N': 0})\n",
        "\n",
        "# 4. Handle Missing Values\n",
        "X = X.fillna(0)\n",
        "X_test_predict = X_test_predict.fillna(0)\n",
        "\n",
        "# --- VARIABLE 2 TO SAVE: Aligned Feature Matrix for Prediction ---\n",
        "# Ensure test columns match training columns exactly (prevents shape mismatch errors)\n",
        "X_test_predict = X_test_predict.reindex(columns=X.columns, fill_value=0)\n",
        "\n",
        "print(f\"--- Final 'X' (Features) DataFrame ---\")\n",
        "print(f\"Shape of X: {X.shape}\")\n",
        "print(f\"Shape of X_test_predict: {X_test_predict.shape}\")\n",
        "print(f\"Features: {X.columns.to_list()}\\n\")\n",
        "\n",
        "print(f\"--- Final 'y' (Target) Series ---\")\n",
        "print(f\"Shape of y: {y.shape}\")\n",
        "print(f\"Class distribution:\\n{y.value_counts()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70f1c1f1",
      "metadata": {},
      "source": [
        "### 5. Exploratory Data Analysis (Correlation)\n",
        "We check for multicollinearity among features. High correlation (e.g., > 0.9) between features can confuse linear models and inflate feature importance in tree models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89df1b53aaa8601c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-10T19:01:53.150838Z",
          "start_time": "2025-11-10T19:01:53.022654Z"
        }
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- C. Check Correlation ---\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "corr_matrix = X.corr().abs()\n",
        "\n",
        "# Create a heatmap\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(corr_matrix, annot=False, cmap='Blues', fmt='.1f')\n",
        "plt.title('Feature Correlation Matrix')\n",
        "plt.show()\n",
        "\n",
        "# You can also manually find high-correlation pairs\n",
        "# Select upper triangle of correlation matrix\n",
        "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "\n",
        "# Find features with correlation greater than 0.9\n",
        "high_corr_features = [column for column in upper_tri.columns if any(upper_tri[column] > 0.9)]\n",
        "\n",
        "if high_corr_features:\n",
        "    print(f\"\\nWARNING!: High Correlation remaining in features: {high_corr_features}\")\n",
        "    print(\"Consider dropping one feature from each correlated pair.\")\n",
        "else:\n",
        "    print(\"\\nNo highly correlated (r > 0.9) features found. Ready for modeling.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcdcf3f5",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "\n",
        "def print_metrics(y_true, y_pred, model_name):\n",
        "    \"\"\"Prints common classification metrics.\"\"\"\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
        "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "    \n",
        "    print(f\"\\n--- {model_name} Metrics ---\")\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "    print(f\"Precision: {prec:.4f}\")\n",
        "    print(f\"Recall: {rec:.4f}\")\n",
        "    print(f\"F1-Score: {f1:.4f}\")\n",
        "    return {\"Model\": model_name, \"Accuracy\": acc, \"Precision\": prec, \"Recall\": rec, \"F1\": f1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64e6decf",
      "metadata": {},
      "outputs": [],
      "source": [
        "test_year = 9\n",
        "starting_year = 3\n",
        "max_year = 9\n",
        "random_state = 45"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bee8732e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import itertools\n",
        "\n",
        "# Validation Strategy: Leave-One-Season-Out\n",
        "years = range(starting_year, max_year + 1)\n",
        "metrics = []\n",
        "\n",
        "for test_year in years:\n",
        "    print(f\"Year: {test_year}\")\n",
        "    # Split data\n",
        "    X_train = X[X['year'] < test_year]\n",
        "    y_train = y[X['year'] < test_year]\n",
        "    X_test = X[X['year'] == test_year]\n",
        "    y_test = y[X['year'] == test_year]\n",
        "\n",
        "    X_train = X_train.drop(columns=['year'])\n",
        "    X_test = X_test.drop(columns=['year'])\n",
        "\n",
        "    # Scale data\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Compute sample weights\n",
        "    sample_weights = compute_sample_weight(class_weight='balanced', y=y_train)\n",
        "\n",
        "    # --- Logistic Regression ---\n",
        "\n",
        "    param_grid_lr = {\n",
        "        'C': [0.01, 0.05, 0.1, 1, 10], \n",
        "        'class_weight': ['balanced']\n",
        "    }\n",
        "\n",
        "    lr = LogisticRegression(max_iter=1000, random_state=random_state)\n",
        "\n",
        "    grid_lr = GridSearchCV(lr, param_grid_lr, cv=5, scoring='f1') \n",
        "    grid_lr.fit(X_train_scaled, y_train)\n",
        "    best_lr = grid_lr.best_estimator_\n",
        "\n",
        "    y_pred_lr = best_lr.predict(X_test_scaled)\n",
        "\n",
        "    lr_metrics = print_metrics(y_test, y_pred_lr, 'Logistic Regression')\n",
        "    print(f\"Best LR Params: {grid_lr.best_params_}\")\n",
        "\n",
        "\n",
        "    # --- Random Forest ---\n",
        "\n",
        "    param_grid_rf = {\n",
        "        'max_depth': [3, 5], \n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'class_weight': ['balanced', 'balanced_subsample'],\n",
        "        'min_samples_leaf': [2, 4, 5],\n",
        "        'max_features': ['sqrt']\n",
        "    }\n",
        "\n",
        "    rf = RandomForestClassifier(random_state=42)\n",
        "    grid_rf = GridSearchCV(rf, param_grid_rf, cv=5, scoring='f1', n_jobs=-1)\n",
        "    grid_rf.fit(X_train_scaled, y_train)\n",
        "    best_rf = grid_rf.best_estimator_\n",
        "\n",
        "    y_pred_rf = best_rf.predict(X_test_scaled)\n",
        "\n",
        "    rf_metrics = print_metrics(y_test, y_pred_rf, 'Random Forest')\n",
        "    print(f\"Best RF Params: {grid_rf.best_params_}\")\n",
        "    \n",
        "\n",
        "    # --- Gradient Boosting ---\n",
        "    param_grid_gb = { \n",
        "        'n_estimators': [100, 200, 500, 1000], \n",
        "        'learning_rate': [0.0005, 0.001, 0.01, 0.05], \n",
        "        'max_depth': [2, 3, 5],\n",
        "    }\n",
        "\n",
        "    gb = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "    grid_gb = GridSearchCV(gb, param_grid_gb, cv=5, scoring='f1', n_jobs=-1)\n",
        "    grid_gb.fit(X_train_scaled, y_train, sample_weight=sample_weights)\n",
        "    best_gb = grid_gb.best_estimator_\n",
        "\n",
        "    best_gb.fit(X_train_scaled, y_train, sample_weight=sample_weights)\n",
        "\n",
        "    y_pred_gb = best_gb.predict(X_test_scaled)\n",
        "\n",
        "    gb_metrics = print_metrics(y_test, y_pred_gb, 'Gradient Boosting')\n",
        "    print(f\"Best GB Params: {grid_gb.best_params_}\")\n",
        "\n",
        "\n",
        "    # --- Support Vector Machine ---\n",
        "\n",
        "    param_grid_svm = {\n",
        "        'C': [0.01, 0.1, 1, 10, 50], \n",
        "        'gamma': ['scale', 'auto'], \n",
        "        'kernel': ['sigmoid'],\n",
        "        'class_weight': ['balanced']\n",
        "    }\n",
        "\n",
        "    svm = SVC(probability=True, random_state=42)\n",
        "    grid_svm = GridSearchCV(svm, param_grid_svm, cv=5, scoring='f1', n_jobs=-1)\n",
        "\n",
        "    grid_svm.fit(X_train_scaled, y_train)\n",
        "    best_svm = grid_svm.best_estimator_\n",
        "\n",
        "    y_pred_svm = best_svm.predict(X_test_scaled)\n",
        "\n",
        "    svm_metrics = print_metrics(y_test, y_pred_svm, 'SVM')\n",
        "    print(f\"Best SVM Params: {grid_svm.best_params_}\")\n",
        "\n",
        "\n",
        "    # --- Decision Tree (Single, Tuned) ---\n",
        "\n",
        "    param_grid_dt = {\n",
        "        'max_depth': [3, 5, 7], \n",
        "        'min_samples_leaf': [1, 2, 4], \n",
        "        'class_weight': ['balanced']\n",
        "    }\n",
        "\n",
        "    dt = DecisionTreeClassifier(random_state=42)\n",
        "    grid_dt = GridSearchCV(dt, param_grid_dt, cv=5, scoring='f1', n_jobs=-1)\n",
        "    grid_dt.fit(X_train_scaled, y_train)\n",
        "    best_dt = grid_dt.best_estimator_\n",
        "\n",
        "    y_pred_dt = best_dt.predict(X_test_scaled)\n",
        "\n",
        "    dt_metrics = print_metrics(y_test, y_pred_dt, 'Decision Tree')\n",
        "    print(f\"Best DT Params: {grid_dt.best_params_}\")\n",
        "\n",
        "    voting_clf = VotingClassifier(\n",
        "        estimators=[\n",
        "            ('lr', best_lr), \n",
        "            ('rf', best_rf), \n",
        "            ('gb', best_gb), \n",
        "            ('svm', best_svm),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    weights_list = [\n",
        "        [1, 1, 1, 1], # Baseline (Equal)\n",
        "        [1, 2, 2, 1], # Favor Trees (Precision)\n",
        "        [2, 1, 1, 2], # Favor SVM & LR (Recall)\n",
        "        [0, 2, 2, 1], # Drop LR, favor Trees\n",
        "        [0, 1, 1, 1], # Drop LR, Equal others\n",
        "        [1, 3, 3, 1], # Aggressively favor Trees\n",
        "        [1, 2, 1, 2]  # RF + SVM partnership\n",
        "    ]\n",
        "\n",
        "    param_grid_voting = {\n",
        "        'weights': weights_list,\n",
        "        'voting': ['soft']\n",
        "    }\n",
        "\n",
        "    grid_voting = GridSearchCV(\n",
        "        estimator=voting_clf,\n",
        "        param_grid=param_grid_voting,\n",
        "        cv=10,\n",
        "        scoring='recall',\n",
        "        n_jobs=-1,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    print(\"Tuning Voting Classifier Weights...\")\n",
        "    grid_voting.fit(X_train_scaled, y_train)\n",
        "\n",
        "    best_voting_clf = grid_voting.best_estimator_\n",
        "    print(f\"\\nBest Weights: {grid_voting.best_params_}\")\n",
        "\n",
        "    y_pred_voting = best_voting_clf.predict(X_test_scaled)\n",
        "\n",
        "    # clf_metrics = print_metrics(y_test, y_pred_voting, 'Tuned Voting Classifier')\n",
        "\n",
        "    all_predictions = {'Logistic Regression': y_pred_lr, 'Random Forest': y_pred_rf, 'Gradient Boosting': y_pred_gb, 'SVM': y_pred_svm, 'Decision Tree': y_pred_dt, 'Voting Classifier': y_pred_voting}\n",
        "    all_metrics = [lr_metrics, rf_metrics, gb_metrics, svm_metrics, dt_metrics]\n",
        "    final_results_df = pd.DataFrame(all_metrics).set_index('Model').sort_values(by='F1', ascending=False)\n",
        "\n",
        "    print(f\"\\n--- Final Metrics Comparison (Test Year {test_year}) ---\")\n",
        "    print(final_results_df)\n",
        "\n",
        "    right_predictions = {}\n",
        "    wrong_predictions = {}\n",
        "\n",
        "    for name, y_pred in all_predictions.items():\n",
        "        # Predict & Rank\n",
        "        test_data = train_df[train_df['year'] == test_year].copy()\n",
        "        test_data['PredictedCoachChange'] = y_pred\n",
        "        \n",
        "        results = test_data[['tmID', 'PredictedCoachChange', 'CoachChange']].copy()\n",
        "        \n",
        "        TP_lines = results[(results['PredictedCoachChange'] == 1) & (results['CoachChange'] == 1)]\n",
        "        TN_lines = results[(results['PredictedCoachChange'] == 0) & (results['CoachChange'] == 0)]\n",
        "\n",
        "        right_predictions[test_year] = {\n",
        "            'True Positives': TP_lines,\n",
        "            'True Negatives': TN_lines\n",
        "        }\n",
        "\n",
        "        FP_lines = results[(results['PredictedCoachChange'] == 1) & (results['CoachChange'] == 0)]\n",
        "        FN_lines = results[(results['PredictedCoachChange'] == 0) & (results['CoachChange'] == 1)]\n",
        "\n",
        "        wrong_predictions[test_year] = {\n",
        "            'False Positives': FP_lines,\n",
        "            'False Negatives': FN_lines\n",
        "        }\n",
        "\n",
        "        print(f\"--- Year {test_year} Summary ---\")\n",
        "\n",
        "        print(\"\\n## ✅ RIGHT PREDICTIONS\")\n",
        "        print(\"### True Positives (Predicted Change, Actual Change):\")\n",
        "        display(TP_lines)\n",
        "        \n",
        "        print(\"### True Negatives (Predicted NO Change, Actual NO Change):\")\n",
        "        display(TN_lines)\n",
        "        \n",
        "        print(\"\\n\" + \"-\"*30)\n",
        "        \n",
        "        ## ❌ Model Got It WRONG\n",
        "        print(\"## ❌ WRONG PREDICTIONS\")\n",
        "        print(\"### False Positives (Predicted Change, Actual NO Change - Type I Error):\")\n",
        "        display(FP_lines)\n",
        "        \n",
        "        print(\"### False Negatives (Predicted NO Change, Actual Change - Type II Error/Missed):\")\n",
        "        display(FN_lines)\n",
        "\n",
        "        metrics.append({\n",
        "            'Year': test_year,\n",
        "            'Model': name,\n",
        "            'TP': len(TP_lines),\n",
        "            'TN': len(TN_lines),\n",
        "            'FP': len(FP_lines),\n",
        "            'FN': len(FN_lines),\n",
        "            'Precision': TP_lines.shape[0] / (TP_lines.shape[0] + FP_lines.shape[0]),\n",
        "            'Recall': TP_lines.shape[0] / (TP_lines.shape[0] + FN_lines.shape[0]),\n",
        "            'F1': 2 * (TP_lines.shape[0] / (TP_lines.shape[0] + FP_lines.shape[0])) * (TP_lines.shape[0] / (TP_lines.shape[0] + FN_lines.shape[0])) / ((TP_lines.shape[0] / (TP_lines.shape[0] + FP_lines.shape[0])) + (TP_lines.shape[0] / (TP_lines.shape[0] + FN_lines.shape[0])))\n",
        "        })\n",
        "\n",
        "metrics_df = pd.DataFrame(metrics)\n",
        "metrics_df.sort_values(by='F1', ascending=False)\n",
        "display(metrics_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df9090ec",
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics_df = metrics_df.sort_values(by='F1', ascending=False)\n",
        "\n",
        "display(metrics_df[metrics_df['Year'] == 7].style.background_gradient(cmap='Greens', subset=['F1', 'Precision', 'Recall']))\n",
        "display(metrics_df[metrics_df['Year'] == 8].style.background_gradient(cmap='Greens', subset=['F1', 'Precision', 'Recall']))\n",
        "display(metrics_df[metrics_df['Year'] == 9].style.background_gradient(cmap='Greens', subset=['F1', 'Precision', 'Recall']))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
