{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Coach of the Year (COTY) Prediction Pipeline\n",
                "\n",
                "This notebook implements a pipeline to predict the winner of the \"Coach of the Year\" award. \n",
                "The award is typically given to the coach with the best record or the most improved team.\n",
                "\n",
                "## Steps\n",
                "1. **Data Preparation**: Load data and filter for COTY awards.\n",
                "2. **Feature Engineering**: Calculate `win_diff` (improvement) and other metrics.\n",
                "3. **Modeling**: Train a Learning-to-Rank model (Logistic Regression) to predict the winner."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.metrics import accuracy_score, classification_report\n",
                "\n",
                "# Display settings\n",
                "pd.set_option('display.max_columns', None)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "coaches = pd.read_csv('../../initial_data/coaches.csv')\n",
                "awards = pd.read_csv('../../initial_data/awards_players.csv')\n",
                "teams = pd.read_csv('../../initial_data/teams.csv')\n",
                "\n",
                "print(\"Coaches:\", coaches.shape)\n",
                "print(\"Awards:\", awards.shape)\n",
                "print(\"Teams:\", teams.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Entity Resolution & Stint Handling\n",
                "We filter for \"Coach of the Year\" awards and handle coaches who had multiple stints in a single year."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Filter COTY awards\n",
                "coty_awards = awards[awards['award'] == 'Coach of the Year'].copy()\n",
                "coty_awards = coty_awards.rename(columns={'playerID': 'coachID'})\n",
                "coty_awards['is_coty'] = 1\n",
                "\n",
                "# Handle Stints: Aggregate coach stats per year\n",
                "# We prioritize the team where they coached the majority of games\n",
                "coaches['games'] = coaches['won'] + coaches['lost']\n",
                "coaches_agg = coaches.sort_values('games', ascending=False).drop_duplicates(subset=['coachID', 'year'])\n",
                "\n",
                "print(\"Unique Coach-Seasons:\", coaches_agg.shape[0])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Feature Engineering\n",
                "We calculate key predictors:\n",
                "- **win_diff**: Improvement from the previous season (Current Wins - Previous Wins).\n",
                "- **conf_rank**: Conference rank (lower is better)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate Team Metrics (Win Diff)\n",
                "teams_metrics = teams[['tmID', 'year', 'won', 'lost', 'rank', 'seeded']].copy()\n",
                "teams_metrics['win_pct'] = teams_metrics['won'] / (teams_metrics['won'] + teams_metrics['lost'])\n",
                "\n",
                "# Calculate previous year's wins\n",
                "teams_metrics = teams_metrics.sort_values(['tmID', 'year'])\n",
                "teams_metrics['prev_won'] = teams_metrics.groupby('tmID')['won'].shift(1)\n",
                "teams_metrics['win_diff'] = teams_metrics['won'] - teams_metrics['prev_won']\n",
                "teams_metrics['win_diff'] = teams_metrics['win_diff'].fillna(0)\n",
                "\n",
                "# Merge Coach Data with Team Metrics\n",
                "df = pd.merge(coaches_agg, teams_metrics, on=['tmID', 'year'], how='left', suffixes=('', '_team'))\n",
                "\n",
                "# Merge Target Variable\n",
                "df = pd.merge(df, coty_awards[['coachID', 'year', 'is_coty']], on=['coachID', 'year'], how='left')\n",
                "df['is_coty'] = df['is_coty'].fillna(0)\n",
                "\n",
                "# Final Feature Cleanup\n",
                "df['team_won'] = df['won_team']\n",
                "df['team_lost'] = df['lost_team']\n",
                "df['conf_rank'] = df['rank']\n",
                "\n",
                "# Drop rows with missing critical data\n",
                "df = df.dropna(subset=['team_won', 'conf_rank'])\n",
                "\n",
                "print(\"Final Dataset Shape:\", df.shape)\n",
                "df[['coachID', 'year', 'tmID', 'won', 'win_diff', 'conf_rank', 'is_coty']].head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.model_selection import GridSearchCV\n",
                "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.metrics import roc_curve, auc, average_precision_score\n",
                "import xgboost as xgb\n",
                "\n",
                "features = ['won', 'lost', 'win_diff', 'conf_rank']\n",
                "target = 'is_coty'\n",
                "\n",
                "# Split Data\n",
                "max_year = df['year'].max()\n",
                "test_year = max_year - 1\n",
                "\n",
                "print(\"Test Year:\", test_year)\n",
                "\n",
                "train_df = df[df['year'] < test_year].copy()\n",
                "test_df = df[df['year'] >= test_year].copy()\n",
                "\n",
                "X_train = train_df[features]\n",
                "y_train = train_df[target]\n",
                "X_test = test_df[features]\n",
                "y_test = test_df[target]\n",
                "\n",
                "# Scale Features\n",
                "scaler = StandardScaler()\n",
                "X_train_s = scaler.fit_transform(X_train)\n",
                "X_test_s = scaler.transform(X_test)\n",
                "\n",
                "random_state = 99\n",
                "\n",
                "metrics_list = []\n",
                "\n",
                "def print_metrics(y_true, y_pred, model_name):\n",
                "    acc = accuracy_score(y_true, y_pred)\n",
                "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
                "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
                "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
                "    \n",
                "    print(f\"\\n--- {model_name} Metrics ---\")\n",
                "    print(f\"Accuracy: {acc:.4f}\")\n",
                "    print(f\"Precision: {prec:.4f}\")\n",
                "    print(f\"Recall: {rec:.4f}\")\n",
                "    print(f\"F1-Score: {f1:.4f}\")\n",
                "    return {\"Model\": model_name, \"Accuracy\": acc, \"Precision\": prec, \"Recall\": rec, \"F1\": f1}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Logistic Regression\n",
                "param_grid_lr = {\n",
                "    'C': [0.01, 0.1, 1, 10, 100], \n",
                "    'solver': ['liblinear', 'lbfgs']\n",
                "}\n",
                "\n",
                "lr = LogisticRegression(random_state=random_state, class_weight='balanced', max_iter=1000)\n",
                "\n",
                "grid_lr = GridSearchCV(lr, param_grid_lr, cv=5, scoring='f1', n_jobs=-1)\n",
                "grid_lr.fit(X_train_s, y_train)\n",
                "\n",
                "best_lr = grid_lr.best_estimator_\n",
                "y_pred_lr = best_lr.predict(X_test_s)\n",
                "\n",
                "print(f\"Best LR Params: {grid_lr.best_params_}\")\n",
                "metrics_list.append(print_metrics(y_test, y_pred_lr, 'Logistic Regression'))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Random Forest\n",
                "param_grid_rf = {\n",
                "    'n_estimators': [50, 100, 200],\n",
                "    'max_depth': [3, 5, 7],\n",
                "    'min_samples_split': [2, 5],\n",
                "    'max_features': ['sqrt', 'log2']\n",
                "}\n",
                "\n",
                "rf = RandomForestClassifier(random_state=random_state, class_weight='balanced')\n",
                "\n",
                "grid_rf = GridSearchCV(rf, param_grid_rf, cv=5, scoring='f1', n_jobs=-1)\n",
                "grid_rf.fit(X_train_s, y_train)\n",
                "\n",
                "best_rf = grid_rf.best_estimator_\n",
                "y_pred_rf = best_rf.predict(X_test_s)\n",
                "\n",
                "print(f\"Best RF Params: {grid_rf.best_params_}\")\n",
                "metrics_list.append(print_metrics(y_test, y_pred_rf, 'Random Forest'))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Support Vector Machine\n",
                "param_grid_svc = {\n",
                "    'C': [0.001, 0.01, 0.1, 1, 10],\n",
                "    'gamma': ['scale', 'auto'],\n",
                "    'kernel': ['rbf', 'linear', 'sigmoid']\n",
                "}\n",
                "\n",
                "svc = SVC(probability=True, random_state=random_state, class_weight='balanced')\n",
                "\n",
                "grid_svc = GridSearchCV(svc, param_grid_svc, cv=5, scoring='f1', n_jobs=-1)\n",
                "grid_svc.fit(X_train_s, y_train)\n",
                "\n",
                "best_svc = grid_svc.best_estimator_\n",
                "y_pred_svc = best_svc.predict(X_test_s)\n",
                "\n",
                "print(f\"Best SVC Params: {grid_svc.best_params_}\")\n",
                "metrics_list.append(print_metrics(y_test, y_pred_svc, 'SVC'))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. XGBoost\n",
                "best_xgb = xgb.XGBRanker(\n",
                "    objective='rank:pairwise', \n",
                "    learning_rate=0.01, \n",
                "    n_estimators=100, \n",
                "    max_depth=5, \n",
                "    random_state=random_state,\n",
                ")\n",
                "\n",
                "groups_train = train_df.groupby('year').size().to_list()\n",
                "groups_test = test_df.groupby('year').size().to_list()\n",
                "\n",
                "best_xgb.fit(X_train_s, y_train, group=groups_train)\n",
                "\n",
                "y_pred_xgb = best_xgb.predict(X_test_s)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "models = {\n",
                "    'Logistic Regression': best_lr,\n",
                "    'Random Forest': best_rf,\n",
                "    'SVM (RBF)': best_svc,\n",
                "    'XGBRanker': best_xgb\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics import roc_curve, auc\n",
                "\n",
                "predicted_res = {}\n",
                "\n",
                "def calculate_ranking_metrics(model, X, y, df_meta):\n",
                "    \"\"\"\n",
                "    Calculates Top-1, Top-3, Avg Rank, and MRR for a model on a given dataset.\n",
                "    \"\"\"\n",
                "    temp_df = df_meta.copy()\n",
                "    \n",
                "    # Get scores (proba for classifiers, predict for XGBoost)\n",
                "    if hasattr(model, \"predict_proba\"):\n",
                "        temp_df['pred_score'] = model.predict_proba(X)[:, 1]\n",
                "    else:\n",
                "        temp_df['pred_score'] = model.predict(X)\n",
                "        \n",
                "    years = temp_df['year'].unique()\n",
                "    \n",
                "    ranks = []\n",
                "    top1_hits = 0\n",
                "    top3_hits = 0\n",
                "    reciprocal_ranks = []\n",
                "    \n",
                "    for yr in years:\n",
                "        yr_data = temp_df[temp_df['year'] == yr]\n",
                "        \n",
                "        # Skip years with no winner (data integrity check)\n",
                "        if yr_data['is_coty'].sum() == 0:\n",
                "            continue\n",
                "            \n",
                "        # 1. Identify Actual Winner\n",
                "        actual_winner_id = yr_data[yr_data['is_coty'] == 1]['coachID'].values[0]\n",
                "        \n",
                "        # 2. Rank Candidates by Score (Highest Score = Rank 1)\n",
                "        yr_data_sorted = yr_data.sort_values('pred_score', ascending=False).reset_index(drop=True)\n",
                "        \n",
                "        # 3. Find Rank of Actual Winner\n",
                "        # We add 1 because index starts at 0\n",
                "        winner_rank = yr_data_sorted[yr_data_sorted['coachID'] == actual_winner_id].index[0] + 1\n",
                "        \n",
                "        ranks.append(winner_rank)\n",
                "        reciprocal_ranks.append(1 / winner_rank)\n",
                "        \n",
                "        if winner_rank == 1:\n",
                "            top1_hits += 1\n",
                "        if winner_rank <= 3:\n",
                "            top3_hits += 1\n",
                "            \n",
                "    # Calculate Averages\n",
                "    n = len(ranks)\n",
                "    metrics = {\n",
                "        'Top-1 Acc': top1_hits / n if n > 0 else 0,\n",
                "        'Top-3 Acc': top3_hits / n if n > 0 else 0,\n",
                "        'Avg Rank': np.mean(ranks) if n > 0 else 0,\n",
                "        'MRR': np.mean(reciprocal_ranks) if n > 0 else 0\n",
                "    }\n",
                "    return metrics\n",
                "\n",
                "results_list = []\n",
                "plt.figure(figsize=(10, 8))\n",
                "\n",
                "train_meta = train_df[['year', 'coachID', 'is_coty']].reset_index(drop=True)\n",
                "\n",
                "for name, model in models.items():\n",
                "    if name == \"XGBRanker\":\n",
                "        y_prob = model.predict(X_test_s)\n",
                "    else:\n",
                "        y_prob = model.predict_proba(X_test_s)[:, 1]\n",
                "        \n",
                "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
                "    roc_auc = auc(fpr, tpr)\n",
                "    \n",
                "    rank_metrics = calculate_ranking_metrics(model, X_train_s, y_train, train_meta)\n",
                "    \n",
                "    # 4. Save Results\n",
                "    results_list.append({\n",
                "        'Model': name,\n",
                "        'AUC (Test)': roc_auc,\n",
                "        'Avg Winner Rank (Train)': rank_metrics['Avg Rank'],\n",
                "        'Top-1 Acc (Train)': rank_metrics['Top-1 Acc'],\n",
                "        'Top-3 Acc (Train)': rank_metrics['Top-3 Acc'],\n",
                "        'MRR (Train)': rank_metrics['MRR']\n",
                "    })\n",
                "    \n",
                "    # Plot ROC\n",
                "    plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {roc_auc:.2f})')\n",
                "\n",
                "# Formatting\n",
                "plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Chance')\n",
                "plt.xlabel('False Positive Rate')\n",
                "plt.ylabel('True Positive Rate')\n",
                "plt.title('ROC Curve Comparison')\n",
                "plt.legend(loc=\"lower right\")\n",
                "plt.show()\n",
                "\n",
                "# --- Display Final Table ---\n",
                "# Sort by Top-1 Acc first, then Avg Rank (lower is better)\n",
                "results_df = pd.DataFrame(results_list)\n",
                "results_df = results_df.sort_values(by=['Avg Winner Rank (Train)', 'AUC (Test)'], ascending=[True, False])\n",
                "\n",
                "print(\"\\n--- Comprehensive Model Evaluation ---\")\n",
                "# Format floats for cleaner reading\n",
                "display(results_df.style.format({\n",
                "    'AUC (Test)': '{:.3f}',\n",
                "    'Top-1 Acc (Train)': '{:.1%}',\n",
                "    'Top-3 Acc (Train)': '{:.1%}',\n",
                "    'Avg Winner Rank (Train)': '{:.1f}',\n",
                "    'MRR (Train)': '{:.3f}'\n",
                "}))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Summary of the Strategy**\n",
                "- High Train Ranking + High Test AUC = Great Model. (It learned the history well and applies it well to the future).\n",
                "\n",
                "- High Train Ranking + Low Test AUC = Overfitting. (It memorized the history but fails on new data).\n",
                "\n",
                "- Low Train Ranking + High Test AUC = Underfitting/Luck. (It didn't learn the history well, but got lucky on the test year)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "winning_predicts = pd.DataFrame()\n",
                "winning_predicts['Model'] = \"\"\n",
                "winning_predicts['Top 1'] = np.nan\n",
                "winning_predicts['Top 3'] = np.nan\n",
                "winning_predicts['Missed'] = np.nan\n",
                "winning_predicts['Avg Winner Rank'] = np.nan\n",
                "\n",
                "\n",
                "for name, model in models.items():\n",
                "    full_viz_df = test_df.copy()\n",
                "\n",
                "    # Score\n",
                "    if name == \"XGBRanker\":\n",
                "        full_viz_df['score'] = model.predict(X_test_s)\n",
                "    else:\n",
                "        full_viz_df['score'] = model.predict_proba(X_test_s)[:, 1]\n",
                "\n",
                "    full_viz_df.sort_values('year').reset_index(drop=True)\n",
                "\n",
                "    # Year-by-Year Analysis\n",
                "    unique_years = sorted(full_viz_df['year'].unique())\n",
                "    cols_to_show = ['Predicted_Rank', 'coachID', 'tmID', 'won', 'win_diff', 'score']\n",
                "\n",
                "    missed = 0\n",
                "    top_1 = 0\n",
                "    top_3 = 0\n",
                "    avg_rank = 0\n",
                "\n",
                "    print(f\"--- Year-by-Year Precision Analysis: {name} ---\")\n",
                "\n",
                "    for yr in unique_years:\n",
                "        yr_data = full_viz_df[full_viz_df['year'] == yr].copy()\n",
                "        \n",
                "        # Create Rankings\n",
                "        yr_data_sorted = yr_data.sort_values('score', ascending=False).reset_index(drop=True)\n",
                "        yr_data_sorted['Predicted_Rank'] = yr_data_sorted.index + 1\n",
                "        \n",
                "        # Find Actual Winner\n",
                "        winner_row = yr_data[yr_data['is_coty'] == 1]\n",
                "        \n",
                "        if not winner_row.empty:\n",
                "            winner_id = winner_row['coachID'].values[0]\n",
                "            winner_team = winner_row['tmID'].values[0]\n",
                "            \n",
                "            # Find Model's Rank\n",
                "            pred_rank = yr_data_sorted[yr_data_sorted['coachID'] == winner_id]['Predicted_Rank'].values[0]\n",
                "\n",
                "            avg_rank += pred_rank\n",
                "            \n",
                "            # Status Icon\n",
                "            if pred_rank == 1:\n",
                "                top_1 += 1\n",
                "                status = \"✅ PERFECT (Rank 1)\"\n",
                "            elif pred_rank <= 3:\n",
                "                top_3 += 1\n",
                "                status = f\"⚠️ Top 3 (Rank {pred_rank})\"\n",
                "            else:\n",
                "                missed += 1\n",
                "                status = f\"❌ Missed (Rank {pred_rank})\"\n",
                "                \n",
                "            print(f\"\\nYear {yr}: \\n\\tWinner = {winner_id} ({winner_team}) | {status}\")\n",
                "            \n",
                "            # Display Top Prediction vs Winner (if different)\n",
                "            if pred_rank > 1:\n",
                "                print(\"\\tModel Pick:\")\n",
                "                display(yr_data_sorted.head(1)[cols_to_show])\n",
                "                print(\"\\tActual Winner:\")\n",
                "                display(yr_data_sorted[yr_data_sorted['coachID'] == winner_id][cols_to_show])\n",
                "            else:\n",
                "                display(yr_data_sorted.head(1)[cols_to_show])\n",
                "                \n",
                "        else:\n",
                "            print(f\"\\nYear {yr}: [No Winner Recorded]\")\n",
                "        \n",
                "    avg_rank /= len(unique_years)\n",
                "    winning_predicts.loc[name,'Model'] = name\n",
                "    winning_predicts.loc[name,'Top 1'] = top_1\n",
                "    winning_predicts.loc[name,'Top 3'] = top_3\n",
                "    winning_predicts.loc[name,'Missed'] = missed\n",
                "    winning_predicts.loc[name,'Avg Winner Rank'] = avg_rank\n",
                "\n",
                "\n",
                "winning_predicts = winning_predicts.merge(results_df[['Model', 'AUC (Test)']], on='Model', how='left')\n",
                "winning_predicts = winning_predicts.sort_values(['Top 1', 'Avg Winner Rank', 'AUC (Test)'], ascending=[False, True, False]).reset_index(drop=True)\n",
                "\n",
                "print(\"\\n--- Final Year-by-Year Precision Analysis ---\")\n",
                "display(winning_predicts)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "best_model_name = winning_predicts.iloc[0]['Model']\n",
                "\n",
                "full_history_df = df.copy().sort_values('year').reset_index(drop=True)\n",
                "\n",
                "# Define Features and Target\n",
                "X_full = full_history_df[features]\n",
                "y_full = full_history_df[target]\n",
                "\n",
                "groups_full = full_history_df.groupby('year', sort=False).size().to_list()\n",
                "\n",
                "# Train final model\n",
                "final_model = models[best_model_name]\n",
                "\n",
                "# XGBRanker needs group info\n",
                "if final_model is models['XGBRanker']:\n",
                "    final_model.fit(X_full, y_full, group=groups_full)\n",
                "else:\n",
                "    final_model.fit(X_full, y_full)\n",
                "\n",
                "print(f\"Final Model ({best_model_name}) trained on full history (Years 1-10). Ready for Year 11 inputs.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
